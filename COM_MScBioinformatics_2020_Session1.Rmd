---
title: "MSc Bioinformatics - Probability theory & statistical concepts"
author: "Marc Henrion"
date: "13 January 2020"
output:
  powerpoint_presentation:
    reference_doc: COM_Stats_Template.pptx
---

```{r setup, include=FALSE, echo=F}
knitr::opts_chunk$set(echo = TRUE, fig.width=16, fig.height=9, dpi=150, highlight=T)

require(tidyverse)
require(knitr)
require(gridExtra)
```

# Session 1: Probability theory & statistical concepts

## Preliminaries

In this session we will (re)view the basic fundaments of probability and statistical theory that you need to know for later sessions.

$$\,$$

* Participant packs
  + [https://github.com/gitMarcH/COM_STA621_2020](https://github.com/gitMarcH/COM_STA621_2020)
  + Copy of slides & some R codes.


* Note
  + These slides have been generated by using R Markdown.


## Outline

* Statistics & uncertainty
* Probability theory
* Random variables
* Common distributions
* Central Limit Theorem
* Simulating data (using R)


# Statistics & uncertainty

## Statistics & uncertainty

Flip a (balanced) coin: the probability of H or T is exactly 0.5.

Flip the coin 10 times:

* you would expect 5 H and 5 T
* yet sometimes you will see 3 H, sometimes 6 H, sometimes 5 H, ...
* there is inherent **uncertainty** in the experiment

$$\,$$

```{r}
H<-rbinom(5,size=10,prob=0.5)
print(H)
```


## Statistics & uncertainty

Let's repeat this 1000 times:

```{r}
H<-rbinom(1000,size=10,prob=0.5)

tibble(H) %>%
  ggplot(mapping=aes(x=H)) +
    geom_bar() +
    scale_x_continuous(breaks=0:10) +
    coord_cartesian(xlim=c(0,10)) + 
    xlab("number of heads") +
    theme(text = element_text(size=20))
```


## Statistics & uncertainty

Let's continue with this example: suppose we have 2 **biased** coins

* first coin has $P_1(H)=0.3$
* second coint has $P_2(H)=0.6$

```{r}
dat<-data.frame(
  coin=c(rep("coin1",500),rep("coin2",500)),
  H=c(rbinom(500,size=10,prob=0.3),rbinom(500,size=10,prob=0.6))
)
```

## Statistics & uncertainty

Sometimes the coin with a lesser probability of heads yields more heads.

```{r}
dat %>%
  count(coin,H) %>%
  complete(coin,H,fill=list(n=0)) %>%
  ggplot(mapping=aes(fill=coin,y=n,x=H)) +
    geom_bar(position="dodge",stat="identity") +
    scale_x_continuous(breaks=0:10) +
    coord_cartesian(xlim=c(0,10)) +
    xlab("number of heads") +
    ylab("count") +
    theme(text = element_text(size=20))
```

## Statistics & uncertainty

Suppose weight was a deterministic function of a toddler's age: you start off at 3.5kg, and then your weight follows a logarithmic curve until age 24 months at which point you reach 11.5kg.

This is obviously not what happens in real life (though an OK approximation for US girls).

```{r}
ageMonths<-runif(100,min=0,max=24) # sample some random ages
weight<-3.5+2.5*log(ageMonths+1) # compute the weights
dat<-tibble(ageMonths,weight)

dat %>%
  ggplot(mapping=aes(x=ageMonths,y=weight)) +
    geom_point(color="steelblue") +
    xlab("age in months") + ylab("weight in kg") +
    ggtitle("Toddlers' weights versus ages") +
    theme(text = element_text(size=20))
```

## Statistics & uncertainty

Now suppose when we measure a toddler's weight, the accuracy of our measurement is not perfect: each time we record a slightly different value than the true one, though on average we get it right.

```{r}
dat<-mutate(dat,weightWithError=rnorm(nrow(dat),mean=weight,sd=0.25))

dat %>%
  ggplot(mapping=aes(x=ageMonths,y=weightWithError)) +
    geom_point(color="steelblue") +
    xlab("age in months") + ylab("weight in kg") + 
    ggtitle("Toddlers' weights versus ages") +
    theme(text = element_text(size=20))
```


## Statistics & uncertainty

Now suppose we can measure weights perfectly again. But now suppose that there are unobserved characteristics (e.g. genetic factors, nutritional status, ...) that impact on children's weights.


```{r}
dat<-dat %>% 
  mutate(latentFactor1=rnorm(nrow(dat),mean=0,sd=0.5)) %>%
  mutate(latentFactor2=rnorm(nrow(dat),mean=0,sd=0.1)) %>%
  mutate(weightGivenLatentFactors=3.5+latentFactor1+(2.5+latentFactor2)*log(ageMonths+1))

dat %>%
  ggplot(mapping=aes(x=ageMonths,y=weightGivenLatentFactors)) +
    geom_point(color="steelblue") +
    xlab("age in months") + ylab("weight in kg") +
    ggtitle("Toddlers' weights versus ages") +
    theme(text = element_text(size=20))
```

## Statistics & uncertainty

We have seen 3 sources of uncertainty:

* inherent uncertainty / randomness of an experiment
* measurement error
* unobserved relevant variables

$$ \, $$ 

There can be many other sources of uncertainty.


## Statistics & uncertainty

##### PROBABILITY
To quantify uncertainty we need to use the tools of **probability theory**.

##### STATISTICS
To gain insights from observational data we need to make **statistical inference**.


# Probability theory

## Probability theory

This section is largely based on and in places quoted verbatim from

$$ \, $$ 

Feelders, Ad J. (2007), 'Statistical Concepts', in Berthold, M., Hand, D.J. (eds.) *Intelligent Data Analysis*, 2^nd^ ed., Springer, pp.17-68


## Probability theory: random experiments

A **random experiment** is an experiment that satisfies the following conditions:

1. \ All possible outcomes are known in advance.
2. \ In any particular trial, the outcome is not known in advance.
3. \ The experiment can be repeated under identical conditions.

The **outcome space** $\Omega$ of an experiment is the set of all possible outcomes of the experiment.

Examples

* In the coin tossing experiment earlier $\Omega=\{H,T\}$. 
* When you roll a die $\Omega=\{1,2,3,4,5,6\}$.

## Probability theory: random experiments

An **event** is a subset of the outcome space.

Examples

* "Coin lands head": $A = \{x\in\Omega | \,x\mbox{ is heads}\} = \{H\}$
* "Die shows even number": $B = \{x\in\Omega | \,x\mbox{ is even}\} = \{2,4,6\}$

Special events

* Impossible / empty event: $A = \emptyset$ 
* Sure event / outcome space: $A = \Omega$
* Singleton events: $A = \{H\}$, $A = \{3\}$
* The complementary event: $\bar{A}=\Omega\setminus A$


## Probability theory: probability

Classical definition of probability:

Let $|.|$ denote the operator measuring the size of an event. The **probability** of an event $A\subseteq\Omega$ is defined as
$$P(A)=\frac{|A|}{|\Omega|}$$

If all outcomes in $\Omega$ are equally likely, then this means the probability of $A$ is the ratio of the number of outcomes in $A$ and the number of outcomes in $\Omega$.

If your outcome space is not discrete, then $|.|$ is a function mapping outcome sets to the positive real line.


## Probability theory: probability

Probability as a mathematical concept was formally introduced in the 17^th^ century by French mathematicians **Blaise Pascal** and **Pierre de Fermat** when they were discussing games of chance.

$$ \, $$ 

Note: also a frequency and a subjective definition of probability.

$$ \, $$ 

The formal, mathematical derivation of probability theory follows from set theory and measure theory.


## Probability theory: probability

:::::: {.columns}
::: {.column width="50%"}
![Blaise Pascal (public domain / Wikipedia)](images/pascal.jpg)
:::

::: {.column width="50%"}
![Pierre de Fermat (public domain / Wikipedia)](images/fermat.jpg)
:::
::::::


## Probability theory: probability axioms

Probability is a function $P()$ from subsets $A$ of $\Omega$ to the real line satisfying the following axioms:

$$ \, $$
$$ \, $$ 

1. \ $P(A)\geq0$, for any subset $A$ of $\Omega$
2. \ if $A\cap B=\emptyset$, then $P(A\cup B) = P(A) + P(B)$, for any subsets $A, B$ of $\Omega$
3. \ $P(\Omega)=1$

$$ \, $$ 

Everything else in probability theory is derived from these 3 axioms.


## Probability theory: conditional probability

The probability of an event $A$ can be influenced by information about the occurrence of an event $B$. The **conditional probability** of $A$ given $B$, denoted $P(A|B)$, is defined as the probability of event $A$ given that $B$ has occurred.

$$P(A|B) = \frac{P(A\cap B)}{P(B)}$$

for $P(B)>0$.

$$ \, $$

Intuitively: $B$ is the new outcome space.


## Probability theory: independence

Events $A$ and $B$ are said to be **independent**  if the occurrence of one event does not influence the probability of occurrence of the other events.

$$ \, $$

$$P(A|B)=P(A)$$
$$P(B|A)=P(B)$$

$$ \, $$

This can more concisely be expressed as:

$$P(A\cap B)=P(A)P(B)$$

## Probability theory: Law of Total Probability

We define events $B_1,B_2,\ldots,B_n\subseteq\Omega$ to form a **partition** of $\Omega$ if 

$$\,$$

* $B_i\cap B_j=\emptyset$, $\forall i\neq j$
* $\bigcup_{i=1}^n B_i = \Omega$

$$\,$$

From the probability axioms it follows that, for any event $A$ in $\Omega$, $P(A)$ can be written as a sum:
$$\,$$

$$P(A) = \sum_{i=1}^n {P(A|B_i)\,P(B_i)}=\sum_{i=1}^n {P(A\cap B_i)}$$
This is known as the **Theorem of Total Probability**.


## Probability theory: Law of Total Probability

Example:

A box contains 4 balls: 3 white, 1 red.

First draw one ball at random. Then, without replacing the first ball, draw a second ball from the box.

What is the probability that the second ball is a red ball?


## Probability theory: Law of Total Probability

This is most easily calculated using the TTP.

$$\,$$

Let $R_1, R_2$ be the event of drawing a red ball on the first / second draw, and similarly for $W_1, W_2$.

Note that $\bar{R}_1=W_1$, and hence $R_1, W_1$ form a parition of $\Omega$.

$$\,$$

$$P(R_2)=P(R_2|W_1)P(W_1)+P(R_2|R_1)P(R_1)=\frac{1}{3}\cdot\frac{3}{4}+0\cdot\frac{1}{4}=\frac{1}{4}$$

## Probability theory: Bayes' Rule

Bayes' rule shows how probabilities change in light of evidence. First formulated by an 18^th^ century English clergyman, Thomas Bayes, it was only published after his death.


$$\,$$

$$P(B|A)=\frac{P(A|B)P(B)}{P(A)}$$

And for a partition $B_1,\ldots,B_n$ of $\Omega$:
$$\,$$

$$P(B_i|A)=\frac{P(A|B_i)P(B_i)}{\sum_j{P(A|B_j)P(B_j)}}$$

## Probability theory: Bayes' Rule

![Thomas Bayes (public domain / Wikipedia)](images/bayes.gif)


## Probability theory: Bayes' Rule

Example: diagnostic test

Disease $D$, with $P(D)=0.001$, i.e. occurs only in $0.1\%$ of the population.

There is a diagnostic test, which can give a positive ($T$) or negative ($\bar{T}$) result. The diagnostic test has $95\%$ sensitivity (i.e. $P(T|D)=0.95$) and $98\%$ specificity (i.e. $P(\bar{T}|\bar{D})=0.98$).

$$\,$$

What is the probability that a patient has the disease if the test result is positive?

## Probability theory: Bayes' Rule

Note that $D, \bar{D}$ is a partition of the outcome space.

$$\,$$

Apply Bayes's Rule:
$$
\begin{align}
P(D|T) &= \frac{P(T|D)P(D)}{P(T|D)P(D)+P(T|\bar{D})P(\bar{D})} \\
       &= \frac{0.95\cdot 0.001}{0.95\cdot 0.001 + (1-0.98)\cdot(1-0.001)} \\
       &= 0.0454
\end{align}
$$
$$\,$$
```{r}
(0.95*0.001)/(0.95*0.001+(1-0.98)*(1-0.001))
```




# Random variables

## Random variables

A **random variable** $X$ is a function from the outcome space $\Omega$ to the real line:
$$X:\Omega\rightarrow \mathbb{R}$$
$$ \, $$

Example:
Consider the experiment of tossing a coin 2 times:
$$\Omega=\{(H,H),(H,T),(T,H),(T,T)\}$$
The number of heads turning up is a random variable $X$:

$$X((H,H))=2$$
$$X((H,T))=1$$
$$X((T,H))=1$$
$$X((T,T))=0$$


## Random variables: probability distribution

A **probability mass function** (pmf) $p$ assigns to each realisation $x$ of a *discrete* random variable X the probability $P(X=x)=p(x)$.

$$\,$$

It follows from the axioms of probability that:

  * $p(x)\geq0$

  * $\sum_{x}{p(x)} = 1$


## Random variables: probability distribution

$$\,$$

```{r}
df<-tibble(x=0:10,pmf=dbinom(0:10,size=10,prob=0.5))

ggplot(data=df,mapping=aes(x=x,y=pmf)) +
  geom_bar(stat="identity") +
  xlab("number of heads") + ylab("probability mass") +
  ggtitle("pmf of the random variable counting numbers of H in 10 coin tosses") +
  scale_x_continuous(breaks=0:10) +
  coord_cartesian(xlim=c(0,10)) +
  theme(text = element_text(size=20))
```

## Random variables: probability distribution

$$\,$$
What about continuous random variables?
$$\,$$
For a continuous random variable $X$, $P(X=x)=0$ for all values of x (the probability of *exactly* realising one value among an infinity of possible values is 0). Hence it makes little sense to define a pmf.

## Random variables: probability distribution

Instead, we will define probabilities as areas under a curve. A **probability density function** (pdf) is a function $p:\mathbb{R}\rightarrow\mathbb{R}^+$ so that
$$P(a<X\leq b)=\int_a^bp(x)dx$$
$$\,$$

It follows from the axioms of probability that $p(x)\geq0$ and $\int_{-\infty}^{\infty}{p(x)dx} = 1$.

$$\,$$
Note that while the axioms of probability imply that in the discrete case, a pmf satisfies $p(x)\leq1$, in the continuous case, a pdf $p(x)$ does not have to be bounded above by 1.


## Random variables: probability distribution

$$\,$$

```{r}
x<-seq(-4,4,by=0.01)
xArea <- seq(-2,0.5,by=0.01)
yArea <- dnorm(xArea)

par(mar=c(5,5,5,1))
plot(x, dnorm(x), main="pdf of the standard normal", xlab="x", ylab="density", type="l", cex.lab=2.5,cex.axis=2.5,cex.main=2.5) 
polygon(c(-2,xArea,0.5),c(0,yArea,0),col='steelblue',lty=0)
text(cex=2.5,"P(-2<X<0.5)=0.67",x=-2.5,y=0.25,col="steelblue")
```


## Random variables: probability distribution

Example:

If we have the pdf given by

$$
p(x)=\begin{cases}
 2 & \mbox{ for } 0\leq x\leq 0.5 \\
 0 & \mbox{ otherwise}
\end{cases}
$$
Then it follows that
$$P(0.1<X\leq0.3)=\int_{0.1}^{0.3}2dx=[2x]_{0.1}^{0.3}=0.6-0.2=0.4$$

```{r}
pdf<-function(x){return(ifelse(x>=0 & x<=0.5,2,0))}
integrate(pdf,0.1,0.3)
```

## Random variables: probability distribution

$$\,$$

```{r}
x<-seq(-0.25,0.75,by=0.001)
xArea <- seq(0.1,0.3,by=0.01)
yArea <- dunif(xArea,min=0,max=0.5)

par(mar=c(5,5,5,1))
plot(x, dunif(x,min=0,max=0.5), main="pdf of uniform distribution", xlab="x", ylab="density", type="l", cex.lab=2.5,cex.axis=2.5,cex.main=2.5,ylim=c(0,2.25)) 
polygon(c(0.1,xArea,0.3),c(0,yArea,0),col='steelblue',lty=0)
text(cex=2.5,"P(0.1<X<0.3)=0.4",x=0.2,y=2.1,col="steelblue")
```


## Random variables: expectation & variance

What is the expected or average / mean value for a given distribution? Let us define the **expectation** or the **mean** of a random value.

Discrete random variables:
$$E[X]=\sum_{x}{x\,p(x)}$$
Continuous random variables:
$$E[X]=\int_{-\infty}^{\infty}{x\,p(x)\,dx}$$
Notation:
$$\mu=E[X]$$


## Random variables: expectation & variance

We can also compute expectations for arbitrary functions $h:\mathbb{R}\rightarrow\mathbb{R}$ of a random variable:

$$\,$$

$$
E[h(X)]=\begin{cases}
\sum_x{h(x)\,p(x)} &\mbox{ if }x\mbox{ is discrete} \\
\int_{-\infty}^{\infty}{h(x)\,p(x)\,dx} &\mbox{ if }x\mbox{ is continuous}
\end{cases}
$$


## Random variables: expectation & variance

One special case of such a function $h$ is $h(x)=(x-\mu)^2$ and is used to define the variance of a random variable.

The **variance** $Var(X)=\sigma^2$ of a random variable $X$ is defined as spread around the mean and obtained by averaging the squared differences $(x-\mu)^2$.
$$
\begin{align}
\sigma^2 &=& E[(X-\mu)^2] \\
         &=& E[(X-E[X])^2]
\end{align}
$$
The **standard deviation** $\sigma$ has the advantage of being on the same scale as $X$.


## Random variables: expectation & variance

In summary:

* The **expectation** $E[X]$ characterises the *central tendency* / *mean* / *average* of a distribution.

* The **variance** $Var(X)$ characterises the *variability* /  *spread* of a distribution.

$$\,$$
They are usually the most important properties of a distribution, but they do not say anything about the *shape* of the distribution (higher moments needed to look at skew or kurtosis of a distribution).


## Random variables: conditional distributions

Discrete case:
$$p(x|C)=P(X=x|C)=\frac{P(\{X=x\}\cap C)}{P(C)}$$
$$\,$$
Continuous case:
$$
p(x|C) = 
\begin{cases}
p(x)/P(C) &\mbox{ for }x\in C \\
0         &\mbox{ otherwise}
\end{cases}
$$

## Random variables: joint distributions

A pair of random variables $(X,Y)$ will have a joint distribution and this is uniquely determined by their **joint probability function** $p:\mathbb{R}^2\rightarrow\mathbb{R}$.

Discrete case:
$$p(x,y) = P((X,Y)=(x,y)) = P(X=x, Y=y)$$

From the axioms of probability: $p(x,y)\geq0$ and $\sum_x\sum_y p(x,y) = 1$.

Continuous case:
$$P(a<X\leq b,c<Y\leq d) = \int_a^b\int_c^d p(x,y)dxdy$$

From the axioms of probability: $p(x,y)\geq0$ and $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} p(x,y)\,dxdy = 1$.


## Random variables: marginal probability

The **marginal probability function** of X can be obtained from the joint distribution function by summing (discrete case) or integrating (continuous case) over Y.

$$\,$$
Discrete case:
$$p_X(x)=P(X=x)=\sum_y P((X,Y)=(x,y))=\sum_y p(x,y)$$
$$\,$$
Continuous case:
$$p_X(x)=\int_{-\infty}^{\infty}p(x,y)\,dy$$

$$\,$$


## Random variables: conditional probability

We can define the **conditional** distribution function of X given Y.

$$\,$$
$$p(x|y)=\frac{p(x,y)}{p_Y(y)}$$

$$\,$$
As before for events, we define random variables $X,Y$ to be independent if
$$p(x,y)=p_X(x)p_Y(y)\mbox{ for all }(x,y)$$


# Common distributions

## Common distributions

Parametric distributions depend on parameters who completely define the shape and properties of the distribution. Parametric distributions are important because many arise naturally and because they provide an easy way to model real-world data.

For any given distribution it is important to know its

* Support (what range of values are possible?)

* Shape (bell-shaped, flat, skew, symmetric, ...?)

* Mean / median / mode (central tendancy)
 
* Variance (how much spread in values does it allow?)

We will look at discrete distributions first, then continuous distributions.


## Common distributions: discrete uniform

$X\sim\mbox{ Unif}(k,n)$ if

$$
P(X=x)=\begin{cases}
1/k &\mbox{ if }x\in\{a,a+1,\ldots,a+k-1\} \\
0 &\mbox{ otherwise}
\end{cases}
$$
$$\,$$

$$E(X)=a+\frac{k-1}{2}, \, Var(X)=\frac{k^2-1}{12}$$
$$\,$$

This distribution occurs when there are k equally likely outcomes, each of which occurs with probability $1/k$.

Example:

Roll a die

## Common distributions: discrete uniform

```{r}
plotDUnif<-function(k,col,maxK=10){
  x<-seq(1,k,by=1)
  px<-rep(1/k,k)
  df<-tibble(x=x,px=px)
  
  g<-ggplot(data=df,mapping=aes(x=x,y=px)) +
    geom_bar(stat="identity",fill=col) +
    ylab("P(X=x)") + theme(text = element_text(size=20)) +
    ggtitle(paste(sep="","Discrete uniform with a=1, k=",k)) +
    scale_x_continuous(breaks=1:maxK) +
    coord_cartesian(xlim=c(0.5,maxK+0.5),ylim=c(0,1))
  
  return(g)
}
  
g1<-plotDUnif(1,"darkgrey"); g2<-plotDUnif(2,"steelblue")
g3<-plotDUnif(5,"salmon"); g4<-plotDUnif(10,"orange")

grid.arrange(g1,g2,g3,g4)
```


## Common distributions: Bernoulli

$X\sim\mbox{ Bern}(p)$ if

$$
P(X=x)=\begin{cases}
p &\mbox{ if }x=1 \\
1-p &\mbox{ if }x=0 \\
0 &\mbox{ otherwise}
\end{cases}
$$
$$\,$$

$$E(X)=p,\, Var(X)=p(1-p)$$
$$\,$$

This distribution occurs when there are only 2 outcomes, one of which occurs with probability $p$.

Example:

Coin tossing experiment


## Common distributions: Bernoulli

```{r}
plotBern<-function(p,col){
  x<-seq(0,1,by=1)
  px<-dbinom(x,size=1,prob=p)
  df<-tibble(x=x,px=px)
  
  g<-ggplot(data=df,mapping=aes(x=x,y=px)) +
    geom_bar(stat="identity",fill=col) +
    ylab("P(X=x)") + theme(text = element_text(size=20)) +
    ggtitle(paste(sep="","Bernoulli with p=",p)) +
    scale_x_continuous(breaks=0:1) +
    coord_cartesian(xlim=c(-0.5,1.5),ylim=c(0,1))
  
  return(g)
}
  
g1<-plotBern(0,"darkgrey"); g2<-plotBern(0.25,"steelblue")
g3<-plotBern(0.5,"salmon"); g4<-plotBern(0.9,"orange")

grid.arrange(g1,g2,g3,g4)
```


## Common distributions: Bernoulli

![Jacob Bernoulli (public domain / Wikipedia)](images/bernoulli.jpg)

## Common distributions: Binomial

$X\sim\mbox{ Bin}(n,p)$ if

$$
P(X=x)=\begin{cases}
\binom{n}{x}\,p^x\,(1-p)^{n-x} &\mbox{ if }x\in\{0,1,\ldots,n\} \\
0                              &\mbox{ otherwise}
\end{cases}
$$

$$\,$$

$$E(X)=np,\, Var(X)=np(1-p)$$

The special case $n=1$ is the Bernoulli distribution. Also if $X_1,X_2,\ldots,X_n\sim_{iid}\mbox{ Bern(p)}$, then $Y=\sum_{i=1}^n{X_i} \sim\mbox{ Bin}(n,p)$.

It is the distribution that arises if you count the number of successes among $n$ independent Bernoulli trials.

Example:
Counting the number of heads in 10 coin tosses.


## Common distributions: Binomial

```{r}
plotBin<-function(n,p,col,maxN=10){
  x<-seq(0,n,by=1)
  px<-dbinom(x,size=n,prob=p)
  df<-tibble(x=x,px=px)
  
  g<-ggplot(data=df,mapping=aes(x=x,y=px)) +
    geom_bar(stat="identity",fill=col) +
    ylab("P(X=x)") + theme(text = element_text(size=20)) +
    ggtitle(paste(sep="","Binomial with n=",n,", p=",p)) +
    scale_x_continuous(breaks=0:maxN) +
    coord_cartesian(xlim=c(-0.5,maxN+0.5),ylim=c(0,1))
  
  return(g)
}
  
g1<-plotBin(1,.2,"darkgrey"); g2<-plotBin(2,.2,"steelblue")
g3<-plotBin(3,.2,"salmon"); g4<-plotBin(4,.2,"greenyellow")
g5<-plotBin(5,.2,"mediumorchid"); g6<-plotBin(10,.2,"orange")
grid.arrange(g1,g2,g3,g4,g5,g6,ncol=3)
```


## Common distributions: Multinomial

$$\,$$
You can generalise the binomial to events with more than 2 outcomes.

$$\,$$
This yields the multinomial distribution.


## Common distributions: Poisson

$X\sim\mbox{ Poisson}(\lambda)$ if

$$
P(X=x)=\begin{cases}
\frac{\lambda^xe^{-\lambda}}{x!} &\mbox{ if }x\in\{0,1,2,3\ldots\} \\
0                              &\mbox{ otherwise}
\end{cases}
$$
$$\,$$

$$E(X)=\lambda,\, Var(X)=\lambda$$

This distribution arises if you count the number of events that occur during a fixed interval, with $\lambda$ the rate with which events occur.

The Poisson distribution is a limiting case of the Binomial:

Let $X_n\sim\mbox{ Bin}(n,p)$ and $Y=\mbox{lim}_{n\rightarrow\infty,p\rightarrow 0}X_n$.

If $np$ is kept fixed, then $Y\sim\mbox{ Poisson}(\lambda)$ with rate $\lambda=np$.


## Common distributions: Poisson

Examples:

$$\,$$

* number of mutations on strand of DNA per unit time,
* number of asthma patient arrivals within a given hour of a walk-in clinic
* number of births, deaths, ..., over a given period
* number of photons hitting a telescope image sensor
* ...


## Common distributions: Poisson

![Sim?on Denis Poisson (public domain / Wikipedia)](images/poisson.jpg)


```{r}
plotPois<-function(lambda,col,maxK=10){
  x<-seq(0,maxK,by=1)
  px<-dpois(x,lambda=lambda)
  df<-tibble(x=x,px=px)
  
  g<-ggplot(data=df,mapping=aes(x=x,y=px)) +
    geom_bar(stat="identity",fill=col) +
    ylab("P(X=x)") + theme(text = element_text(size=20)) +
    ggtitle(paste(sep="","Poisson with rate=",lambda)) +
    scale_x_continuous(breaks=0:maxK) +
    coord_cartesian(xlim=c(-0.5,maxK+0.5),ylim=c(0,1))
  
  return(g)
}
  
g1<-plotPois(0,"darkgrey"); g2<-plotPois(0.5,"steelblue")
g3<-plotPois(1,"salmon"); g4<-plotPois(2,"greenyellow")
g5<-plotPois(3,"mediumorchid"); g6<-plotPois(5,"orange")
grid.arrange(g1,g2,g3,g4,g5,g6,ncol=3)
```


## Common distributions: Negative binomial

$X\sim\mbox{ NB}(k,p)$ if

$$
P(X=x)=\begin{cases}
\binom{x+k-1}{x}\,p^k\,(1-p)^x &\mbox{ if }x\in\{0,1,2,3\ldots\} \\
0                              &\mbox{ otherwise}
\end{cases}
$$
$$\,$$

$$E(X)=\frac{k(1-p)}{p},\, Var(X)=\frac{k(1-p)}{p^2}$$

This distribution arises if you count the number of failures in a sequence of identical and independent Bernoulli trials, each with probability of success $p$, until $k$ successes have been reached.

This is the parameterisation R is using.


## Common distributions: Negative binomial

There are many alternative formulations / parameterisations for the Negative Binomial, so always check you know what the parameters mean (e.g. count the number of successes until a fixed number of failures have been reached).

Like the Poisson distribution, the negative binomial distribution is used in count data models. Unlike the Poisson it has 2 parameters, so is particularly useful when your counts show signs of overdispersion (i.e. larger variance than mean).

The negative binomial can be derived as a Gamma-Poisson mixture: Poisson distribution with variable rate parameter having a Gamma distribution.


## Common distributions: Negative binomial

Example:

$$\,$$

* number of co-localised distant fragments in a genomic conformation capture experiment.
* read counts per transcript in NGS transcriptomic data
* number of larvae in a field
* ...


## Common distributions: Negative binomial

```{r, fig.width=22}
plotNB<-function(k,p,col,maxK=25){
  x<-seq(0,maxK,by=1)
  px<-dnbinom(x,size=k,prob=p)
  df<-tibble(x=x,px=px)
  
  g<-ggplot(data=df,mapping=aes(x=x,y=px)) +
    geom_bar(stat="identity",fill=col) +
    ylab("P(X=x)") + theme(text = element_text(size=20)) +
    ggtitle(paste(sep="","Negative binomial with k=",k,", p=",p)) +
    scale_x_continuous(breaks=0:maxK) +
    coord_cartesian(xlim=c(-0.5,maxK+0.5),ylim=c(0,1))
  
  return(g)
}
  
g1<-plotNB(0,0.25,"darkgrey"); g2<-plotNB(1,0.25,"steelblue")
g3<-plotNB(3,0.25,"salmon"); g4<-plotNB(0,0.6,"greenyellow")
g5<-plotNB(1,0.6,"mediumorchid"); g6<-plotNB(3,0.6,"orange")
grid.arrange(g1,g2,g3,g4,g5,g6,ncol=3)
```


## Common distributions: Geometric

$X\sim\mbox{ Geo}(p)$ if

$$
P(X=x)=\begin{cases}
p\,(1-p)^{x-1} &\mbox{ if }x\in\{0,1,2,3\ldots\} \\
0                              &\mbox{ otherwise}
\end{cases}
$$
$$\,$$

$$E(X)=\frac{1}{p},\, Var(X)=\frac{1-p}{p^2}$$

This distribution arises if you count the number of trials until first success.

This is really just a special case of the Negative Binomial (for R's parameterisation of the negative binomial, NB($1,p$)=Geo($1-p$)).


## Common distributions: Uniform

$X\sim\mbox{ Unif}(a,b)$ if

$$
p(x)=\begin{cases}
\frac{1}{b-a} &\mbox{ if }x\in[a,b] \\
0             &\mbox{ otherwise}
\end{cases}
$$

$$\,$$

$$E(X)=\frac{b-a}{2},\, Var(X)=\frac{(b-a)^2}{12}$$

This is the analogue, in the continuous case, of the discrete uniform.

For $a=0,\, b=1$ the distribution is often called the standard uniform. This is very important in sampling theory and algorithms.


## Common distributions: Uniform

Any continuous distribution can be transformed to a uniform distribution by taking $F(X)=P(X\leq x)$, the cumulative distribution function. $F(X)$ has a standard uniform distribution, whatever the distribution of $X$. this is important for copulas - a general technique for modelling joint distributions.

Example:

Take any statistical test involving a continuous distribution. If the null hypothesis is true, then the p-values of the test are standard uniformly distributed.


## Common distributions: Uniform

```{r, fig.width=22}
plotUnif<-function(a,b,col,plotRange=c(-1,6)){
  x<-seq(plotRange[1],plotRange[2],length=1000)
  px<-dunif(x,min=a,max=b)
  df<-tibble(x=x,px=px)
  
  g<-ggplot(data=df,mapping=aes(x=x,y=px)) +
    geom_line(colour=col,lwd=1.5) +
    geom_area(fill=col,alpha=0.25) +
    ylab("p(x)") + theme(text = element_text(size=20)) +
    ggtitle(paste(sep="","Uniform with a=",a,", b=",b)) +
    coord_cartesian(xlim=plotRange,ylim=c(0,2))
  
  return(g)
}
  
g1<-plotUnif(0,0.5,"darkgrey"); g2<-plotUnif(0,1,"steelblue")
g3<-plotUnif(0,2,"salmon"); g4<-plotUnif(1,5,"orange")
grid.arrange(g1,g2,g3,g4,ncol=2)
```


## Common distributions: Exponential

$X\sim\mbox{ Exp}(\lambda)$ if

$$
p(x)=\begin{cases}
\lambda e^{-\lambda x}&\mbox{ if }x>0 \\
0             &\mbox{ otherwise}
\end{cases}
$$

$$\,$$

$$E(X)=\frac{1}{\lambda},\, Var(X)=\frac{1}{\lambda^2}$$

The exponential distribution describes the waiting time between consecutive events in a Poisson process.

An important property of the exponential distribution is that it is *memoryless*: $P(X>x+y|X>y)=P(X>x)$.


## Common distributions: Exponential

```{r, warning=F}
l<-1000; x<-seq(0,10,length=l)
p1x<-dexp(x,rate=0.25); p2x<-dexp(x,rate=0.5)
p3x<-dexp(x,rate=1); p4x<-dexp(x,rate=2)
xFull<-rep(x,4); pFull<-c(p1x,p2x,p3x,p4x)
pars<-factor(c(rep(1,l),rep(2,l),rep(3,l),rep(4,l)))
df<-tibble(x=xFull,p=pFull,pars=pars)

cols<-c("darkgrey", "steelblue", "salmon", "orange")
labs<-paste(sep="","k=",c(0.25,0.5,1,2))

ggplot(data=df,mapping=aes(x=x,y=p,colour=pars,fill=pars)) +
  geom_line(lwd=1,alpha=0.75) +
  geom_area(alpha=0.1,position="dodge") +
  scale_color_manual(values=cols) +
  scale_fill_manual(values=cols,labels=labs) +
  labs(fill="parameters") + guides(colour=F) +
  ylab("density") + ggtitle("Exponential distributions") + 
  theme(text = element_text(size=20)) 
```


## Common distributions: Gamma

$X\sim\Gamma(\alpha,\beta)$ if

$$
p(x)=\begin{cases}
\frac{\beta^\alpha x^{\alpha-1}e^{-\beta x}}{\Gamma(\alpha)}     &\mbox{ if }x>0 \\
0             &\mbox{ otherwise}
\end{cases}
$$

where $\Gamma(\alpha)=\int_0^{\infty}z^{\alpha-1}e^{-z}dz$ is the gamma function.

$$\,$$

$$E(X)=\frac{\alpha}{\beta},\, Var(X)=\frac{\alpha}{\beta^2}$$


## Common distributions: Gamma

The gamma distribution arises as a sum of independent exponentially distributed random variables.

It is an important conjugate prior distribution in Bayesian statistics.

$$\,$$

Example:

Inter-spike intervals in neuroscience.


## Common distributions: Gamma

```{r}

l<-1000; x<-seq(0,20,length=l)
p1x<-dgamma(x,shape=1,rate=0.5); p2x<-dgamma(x,shape=1,rate=2)
p3x<-dgamma(x,shape=0.5,rate=1); p4x<-dgamma(x,shape=2,rate=1)
p5x<-dgamma(x,shape=4,rate=1); p6x<-dgamma(x,shape=8,rate=1)
xFull<-rep(x,6); pFull<-c(p1x,p2x,p3x,p4x,p5x,p6x)
pars<-factor(c(rep(1,l),rep(2,l),rep(3,l),rep(4,l),rep(5,l),rep(6,l)))
df<-tibble(x=xFull,p=pFull,pars=pars)

cols<-c("darkgrey", "steelblue", "salmon", "orange", "greenyellow", "mediumorchid")
labs<-c(expression(paste(sep="",alpha,"=1, ",beta,"=0.5")),
        expression(paste(sep="",alpha,"=1, ",beta,"=2")),
        expression(paste(sep="",alpha,"=0.5, ",beta,"=0.5")),
        expression(paste(sep="",alpha,"=2, ",beta,"=1")),
        expression(paste(sep="",alpha,"=4, ",beta,"=1")),
        expression(paste(sep="",alpha,"=8, ",beta,"=1")))
```


## Common distributions: Gamma

```{r, warning=F}
ggplot(data=df,mapping=aes(x=x,y=p,colour=pars,fill=pars)) +
  geom_line(lwd=1,alpha=0.75) +
  geom_area(alpha=0.1,position="dodge") +
  scale_color_manual(values=cols) +
  scale_fill_manual(values=cols,labels=labs) +
  labs(fill="parameters") + guides(colour=F) +
  ylab("density") + ggtitle("Gamma distribution") + 
  theme(text = element_text(size=20)) +
  coord_cartesian(ylim=c(0,1.5))
```


## Common distributions: Beta

$X\sim\beta(\alpha,\beta)$ if

$$
p(x)=\begin{cases}
\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}     &\mbox{ if }x\in[0,1] \\
0             &\mbox{ otherwise}
\end{cases}
$$

where $B(\alpha,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$

$$\,$$

$$E(X)=\frac{\alpha}{\alpha+\beta},\, Var(X)=\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$$


## Common distributions: Beta

A flexible distribution useful to model continuous variables with support on an interval of finite length.

$$\,$$

An important conjugate prior distribution in Bayesian statistics for a number of discrete distributions.


## Common distributions: Beta

```{r}
l<-1000; x<-seq(0,1,length=l)
p1x<-dbeta(x,shape1=0.5,shape2=0.5); p2x<-dbeta(x,shape1=1,shape2=1)
p3x<-dbeta(x,shape1=2,shape2=2); p4x<-dbeta(x,shape1=1,shape2=3)
p5x<-dbeta(x,shape1=2,shape2=5); p6x<-dbeta(x,shape1=5,shape2=1)
xFull<-rep(x,6); pFull<-c(p1x,p2x,p3x,p4x,p5x,p6x)
pars<-factor(c(rep(1,l),rep(2,l),rep(3,l),rep(4,l),rep(5,l),rep(6,l)))
df<-tibble(x=xFull,p=pFull,pars=pars)

cols<-c("darkgrey", "steelblue", "salmon", "orange", "greenyellow", "mediumorchid")
labs<-c(expression(paste(sep="",alpha,"=0.5, ",beta,"=0.5")),
        expression(paste(sep="",alpha,"=1, ",beta,"=1")),
        expression(paste(sep="",alpha,"=2, ",beta,"=2")),
        expression(paste(sep="",alpha,"=1, ",beta,"=3")),
	expression(paste(sep="",alpha,"=2, ",beta,"=5")),
	expression(paste(sep="",alpha,"=5, ",beta,"=1")))
```


## Common distributions: Beta

```{r, warning=F}
ggplot(data=df,mapping=aes(x=x,y=p,colour=pars,fill=pars)) +
  geom_line(lwd=1,alpha=0.75) +
  geom_area(alpha=0.1,position="dodge") +
  scale_color_manual(values=cols) +
  scale_fill_manual(values=cols,labels=labs) +
  labs(fill="parameters") + guides(colour=F) +
  ylab("density") + ggtitle("Beta distribution") + 
  theme(text = element_text(size=20)) +
  coord_cartesian(ylim=c(0,3))

```

## Common distributions: Normal / Gaussian

$X\sim\mathcal{N}(\mu,\sigma^2)$ if

$$
p(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

$$\,$$

$$E(X)=\mu,\, Var(X)=\sigma^2$$

This distribution is very important.

Because of the *Central Limit Theorem*, the normal distribution both arises in many real life situations and plays a major role in statistical inference.


## Common distributions: Normal / Gaussian

The normal distribution is characteristed by a symmetric bell-shaped curve around the mean.

Important special case: $\mathcal{N}(0,1)$, the *standard normal*.

$$\,$$
Example:

A random variable measuring the height of men.


## Common distributions: Normal / Gaussian

```{r}
l<-1000; x<-seq(-5,5,length=l)
p1x<-dnorm(x,mean=0,sd=1); p2x<-dnorm(x,mean=0,sd=2)
p3x<-dnorm(x,mean=0,sd=0.5); p4x<-dnorm(x,mean=2,sd=1)
xFull<-rep(x,4); pFull<-c(p1x,p2x,p3x,p4x)
pars<-factor(c(rep(1,l),rep(2,l),rep(3,l),rep(4,l)))
df<-tibble(x=xFull,p=pFull,pars=pars)

cols<-c("darkgrey", "steelblue", "salmon", "orange")
labs<-c(expression(paste(sep="",mu,"=0, ",sigma,"=1")),
        expression(paste(sep="",mu,"=0, ",sigma,"=2")),
        expression(paste(sep="",mu,"=0, ",sigma,"=0.5")),
        expression(paste(sep="",mu,"=2, ",sigma,"=1")))
```


## Common distributions: Normal / Gaussian

```{r, warning=F}
ggplot(data=df,mapping=aes(x=x,y=p,colour=pars,fill=pars)) +
  geom_line(lwd=1,alpha=0.75) +
  geom_area(alpha=0.25,position="dodge") +
  scale_color_manual(values=cols) +
  scale_fill_manual(values=cols,labels=labs) +
  labs(fill="parameters") + guides(colour=F) +
  ylab("density") + ggtitle("Normal distribution") + 
  theme(text = element_text(size=20))
```

## Common distributions: Normal / Gaussian

:::::: {.columns}
::: {.column width="50%"}
![Carl Friedrich Gauss (public domain / Wikipedia)](images/gauss.jpg)
:::

::: {.column width="50%"}
![Pierre Simon Laplace (public domain / Wikipedia)](images/laplace.jpg)
:::
::::::


## Common distributions: Chi-squared

The sum of $k$ independent, standard normal distributed random variables follows a $\chi^2$ distribution with $k$ degrees of freedom.

$$Z_i\sim_{iid}\mathcal{N}(0,1), i=1,...,k \;\;\;\Rightarrow\;\;\; X=\sum_{i=1}^kZ_i^2\sim\chi_k^2$$

$$\,$$

$$E(X)=k,\, Var(X)=2k$$

$$\,$$

Important in statistical hypothesis testing, more rarely used to directly model data.


## Common distributions: Student's t


This distribution arises in parameter estimation and hypothesis testing.

Let $X_i\sim_{iid}\mathcal{N}(\mu,\sigma^2), i=1,...,n$. Define

$\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i$ and $S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2$

Then
$$T=\frac{\bar{X}-\mu}{S/\sqrt{n}}\sim t_{n-1}$$

## Common distributions: Student's t

The degrees of freedom of the distribution, $\nu=n-1$ above, does not need to be integer valued, but can take any real value $\nu>0$.

$$\,$$

$$E(X)=1\mbox{ if }\nu>1\mbox{ otherwise undefined}$$
$$Var(X)=\nu/(\nu-2)\mbox{ if }\nu>2,\, \infty\mbox{ if }1<\nu\leq 2\mbox{, undefined otherwise}$$

$$\,$$
Like the normal distribution, the t distribution is bell-shaped, but has heavier tails, meaning observations further away from the mean are more likely.


## Common distributions: Student's t

```{r, warning=F}
l<-1000; x<-seq(-8,8,length=l)
p1x<-dt(x,df=0.25); p2x<-dt(x,df=1)
p3x<-dt(x,df=5); p4x<-dt(x,df=100)
xFull<-rep(x,4); pFull<-c(p1x,p2x,p3x,p4x)
pars<-factor(c(rep(1,l),rep(2,l),rep(3,l),rep(4,l)))
df<-tibble(x=xFull,p=pFull,pars=pars)

cols<-c("darkgrey", "steelblue", "salmon", "orange")
labs<-paste(sep="","k=",c(0.25,1,5,100))

ggplot(data=df,mapping=aes(x=x,y=p,colour=pars,fill=pars)) +
  geom_line(lwd=1,alpha=0.75) +
  geom_area(alpha=0.05,position="dodge") +
  scale_color_manual(values=cols) +
  scale_fill_manual(values=cols,labels=labs) +
  labs(fill="parameters") + guides(colour=F) +
  ylab("density") + ggtitle("Student's t distribution") + 
  theme(text = element_text(size=20)) 
```


## Common distributions: Student's t

![William Sealy Gosset, aka "Student" (public domain / Wikipedia)](images/gosset.jpg)


## Common distributions: F

Also a distribution that arises as the null hypothesis of a test statistic.

$$\,$$
If two independent random variables $X_1\sim\chi_{k1}^2$, $X_2\sim\chi_{k2}^2$, then

$$\,$$
$$F=\frac{X_1/k_1}{X_2/k_2}\sim F_{k_1,k_2}$$


## Common distributions: relationships

![Relationships between common distributions. (Casella, G., Berger, R.L. (2002), *Statistical Inference*, 2^nd^ ed., Duxbury)](images/CasellaBerger_DistRels.jpg)



# Central Limit Theorem

## Sampling distributions

Almost all of the studies you will work on will consist of drawing one (or several) samples that permits you to make inferences about a population (or several) of interest.

How you sample from your population will impact your analysis. Objective assessment can only be made for **probability samples**, i.e. for samples where the inclusion probability is known and positive for every unit in the target population. In a **simple random sample** all units are sampled with equal probability from the target population.

For every unit in the sample you observe one or several variables. In probability samples, these can be viewed as random variables.


## Sampling distributions

Usually we are not interested in the individual outcomes of the sample, but rather in some sample statistic. A **statistic** is a function of the sample observations $X_i$ and is therefore itself a random variable.

$$\,$$
Examples

* the sample mean $\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i$ (sample fraction if $X_i$ binary)
* the sample variance $S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2$


## Sampling distributions

The probability distribution of a sample statistic is called its **sampling distribution**.

In practice it can be hard to derive the sampling distribution analytically and we need to rely on approximations of this distibution based on so called **asymptotic** results.

$$\,$$

The most important of these results is the **Central Limit Theorem**.


## Law of Large Numbers

Let $X_1,...,X_n$ be a sequence of iid random variables, with $X_i\sim F$, where F is a distribution with some finite mean $\mu$ and a finite variance $\sigma^2$. Let $\bar{X}_n=\frac{1}{n}\sum_{i=1}^nX_i$ be the sample average.

The **Law of Large Numbers (LLN)** states that as $n\rightarrow\infty$, $\bar{X}_n\rightarrow \mu$

$$\,$$
Note that there is a strong and a weak version of this law, depending on what exactly you mean by "$\rightarrow$" and they differ slightly in necessary conditions for them to hold.


## Central Limit Theorem

While the LLN states that a sample average will get closer and closer to the true mean as the number of samples increases, fluctuating ever more closely around this value, the **Central Limit Theorem (CLT)** goes a step further and describes the distribution of these fluctuations.

Let $X_1,...,X_n$ a sequence of iid random variables, $X_i\sim F$ a distribution with finite mean $\mu$ and finite variance $\sigma^2$. Then

$$\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\overset{D}{\longrightarrow}\mathcal{N}(0,1)$$
where $\overset{D}{\longrightarrow}$ denotes convergence in distribution.

$$\,$$


## Central Limit Theorem

The CLT is central / key in statistics because:

* Any statistic that is a sum of enough random variables with similar distributions will be approximately normally distributed.
* The normal distribution can be used to approximate the sampling distributions of many statistics, and the statistical theory for the normal distribution can be used to derive properties.

$$\,$$

Examples

* Normal approximation to the binomial.
* t test robust against deviations from normality.
* Height distributions of humans.


## Central Limit Theorem

```{r, echo=F}
cols<-c("darkgrey","steelblue","salmon","orange","greenyellow","mediumorchid","lightcyan","brown")
```

```{r}
generateMeans<-function(n=c(1,2,5,10,50,100,500,1000),col=cols,N=1e4,rDistFun,xlim=NULL,...){
    sim<-matrix(nrow=N,ncol=length(n))
    for(j in 1:length(n)){for(i in 1:N){
            sim[i,j]<-mean(rDistFun(n[j],...))
    }}

    par(mar=c(1.5,2.5,1.5,0.5),mfrow=c(length(n),1))
    for(j in 1:length(n)){
        if(length(xlim)<2){
            hist(breaks=100,sim[,j],main=paste(sep="","n = ",n[j]),yaxt="n",col=col[j])
        }else{
            hist(breaks=100,sim[,j],main=paste(sep="","n = ",n[j]),xlim=xlim,yaxt="n",col=col[j])
        }
    }
}
```


## Central Limit Theorem

```{r}
generateMeans(rDistFun=rnorm,xlim=c(-3,3))
```


## Central Limit Theorem

```{r}
generateMeans(rDistFun=rbeta,shape1=0.5,shape2=0.5,xlim=c(0,1))
```


## Central Limit Theorem

```{r}
generateMeans(rDistFun=rbinom,size=2,prob=0.25,xlim=c(0,2))
```


# Simulating data

## Simulating data

You have seen common distributions in this session and how / when they arise.

You have seen commands in R (e.g. `rnorm()`, `rpois()`) to generate draws from specific distributions.

This and your expert knowledge about the types of diseases and populations you study, can allow you to generate artificial data according to a hypothesised model. This is called **simulating** data.

This can be useful in its own right, but most helpful at the planning stages for a study: it can allow you to compute more accurate sample sizes than when relying on simple, approximate formulas (which you will cover tomorrow).


## Simulating data

Example

You plan to run a vaccine trial. You will randomise 800 participants to the placebo and 800 participants to the experimental vaccine arms respectively. Your primary outcome is the number of events requiring hospitalisation over 1 year.

From previous data you expect the control group to have a rate of 0.3 events per person-year and the vaccine group a rate of 0.25 events per person-year. Due to loss-to-follow-up you expect each participant, in each group, to contribute on average 0.87 years of observation time over the study period.


## Simulating data

Example (cont'd)

Simulate data like this 5,000 times and plot the distributions of the estimated event rates in each group.

How many times is the estimated rate in the vaccine group lower than the one in the control group?


## Simulating data

```{r}
simFun<-function(N,r1,r2,n1,n2,obsTime){
  df<-data.frame(treat=factor(c(rep("placebo",n1),rep("vaccine",n2))))
  for(j in 1:N){
    df[,paste(sep="","sim",j)]<-c(rpois(n1,lambda=r1*obsTime),rpois(n2,lambda=r2*obsTime))
  }
  return(df)
}

N<-5e3
df<-simFun(N=N,r1=0.3,r2=0.25,n1=800,n2=800,obsTime=0.87)
estRates<-data.frame(
           treat=factor(c(rep("placebo",N),rep("vaccine",N))),
           rate=c(apply(FUN=mean,MARGIN=2,X=df[df$treat=="placebo",-1]),
                  apply(FUN=mean,MARGIN=2,X=df[df$treat=="vaccine",-1]))/0.87)
```


## Simulating data

```{r}
ggplot(data=estRates,mapping=aes(x=rate,fill=treat)) +
  geom_histogram(bins=50,position="dodge") + 
  ggtitle("Distribution of estimated event rates") +
  theme(text = element_text(size=20)) 

print(sum(estRates$rate[estRates$treat=="vaccine"]<estRates$rate[estRates$treat=="placebo"]))
```


## Simulating data

Exercise

Think about a study you are working on.

Can you think of a model that could generate data that you are collecting / are plannning to collect? Try simulating data for your study.


## Simulating data

* Binary distributions
  + `rbinom(n,size=1,prob)`
  + `sample(size,x=c("cat1","cat2"),prob=c(p,1-p),replace=T)`
  
* Count data
  + `rbinom(n,size,prob)`
  + `rgeom(n,prob)`
  + `rpois(n,lambda)`
  + `rnbinom(n,size,prob)`
  
* Continuous variables
  + `runif(n,min,max)`
  + `rnorm(n,mean,sd)`
  + `rexp(n,rate)`
  + `rt(m,df)`
  + `rgamma(n,shape,rate)`
  + `rbeta(n,shape1,shape2)`

...


##

[end of Session 1]
