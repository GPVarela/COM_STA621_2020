---
title: "MSc Bioinformatics - Data types, summarising & exploring data"
author: "Marc Henrion"
date: "13 January 2020"
output:
  powerpoint_presentation:
    reference_doc: COM_Stats_Template.pptx
---

```{r setup, include=FALSE, echo=F}
knitr::opts_chunk$set(echo = TRUE, fig.width=16, fig.height=9, dpi=150, highlight=T, echo=F, collapse=T, warning=F)

require(tidyverse)
require(knitr)
require(gridExtra)
```


# Session 2: Types of data, summarising & exploring data, measures of association

## Preliminaries

In this session we will look at how to summarise key aspects of a dataset, how to present this and how to explore a new dataset before starting any analysis.

$$\,$$

* Participant packs
  + [https://github.com/gitMarcH/COM_STA621_2020](https://github.com/gitMarcH/COM_STA621_2020)
  + Copy of slides & some R codes.

* Note
  + These slides have been generated by using R Markdown.


# Introduction

## Why summarise & explore data?

* Quality control & data cleaning:
  + Identify data collection / recording issues.
  + Identify severe outliers.

$$\,$$

* Get a feel for the data / precursor to more complex analyses:
  + What are the distributions of the various variables?
  + What analyses / modelling techniques will be appropriate?
  + Any unexpected trends or patterns?
  

## Main methods for summarising & exploring data.

$$\,$$

1. $\,$ Summary statistics & listings

2. $\,$ Frequency & contingency tables

3. $\,$ Basic graphs

4. $\,$ Simple regression & association statistics [Section 7]



# Types of data

## Types of data

There are several ways to classify data into different types:

$$\,$$

* Quantitative & qualitative / categorical data.

* Discrete & continuous data.

* Categorical, ordinal, interval & ratio data.


## Quantitative vs. qualitative data.

* **Quantitative** data are measured by numbers. Arithmetic operation such as addition, subtraction, multiplication, division can be used and yield meaningful results.

$$\,$$

* **Qualitative** or **categorical** data are measured by levels or classes of a categorical variable:
  + Marital status: single, married, divorced, widowed.
  + Colour: red, green, blue.
  + Severity of a disease: low, medium, high.
  + ...

$$\,$$

Note: here *"qualitative"* does not refer to data from a qualitative study.


## Discrete vs. continuous data.

Qualitative / categorical data are always discrete. Quantitative data can be either discrete or continuous.

* **Discrete** data can take either a set of finite values or values among a *countable* set of possible values.
  + Survival status: alive or dead.
  + Number of heads in 2 successive throws of a coin: 0, 1, 2
  + Number of patients attending A&E during a specific time window: 0, 1, 2, 3, 4, ...

* **Continuous** data take values that lie on a *continuum*, e.g. an interval.
  + Height, weight or age of a person.
  + Cost of a medical treatment.
  + IgG concentration.

## Levels of measurement

A data variable has one of 4 levels of measurement:

* **nominal** - classifies observations into different *categories*
  + alive / dead
  + human, fish, goat, bird

* **ordinal** - different categories, but categories are *ordered*
  + low < medium < high
  + bad < good

* **interval** - ordered data with *degree of difference*; ratios not meaningful
  + temperature in centigrade: difference betwen 10^o^C and 20^o^C is the same as between 50^o^C and 60^o^C but 20^o^C not twice as hot as 10^o^C

* **ratio** - interval data with a *unique, non arbitrary zero value*; ratios meaningful
  + temperature in Kelvin
  + length
  
  
## More on categorical variables

A special case of a categorical variable is a **binary** or **dichotomous** variable: such a variable has only 2 levels.

$$\,$$

* alive / dead
* healthy / diseased
* adult / child
* male / female

$$\,$$

Often such variables are coded *numerically*, taking the values 0 and 1. This allows them to be used easily in a regression model, for example.

## More on categorical variables

A multi-level categorical variable with $k$ different levels, can be coded as a set of $k-1$ binary variables, sometime called **dummy variables**.

$$\,$$

* Disease_severity = no disease, mild, severe

* Disease_mild = 0, 1; Disease_severe = 0, 1


## More on cateogrical variables

Continuous variables can be categorised or dichotomised:

* BMI continuous, taking values $\geq0$
  + underweight ($<18.5\,kg/m^2$), normal weight ($\geq18.5,<25\,kg/m^2$)
  + overweight ($\geq25, <30\,kg/m^2$), obese ($\geq30\,kg/m^2$)

* Plasma glucose, continuous, taking values $\geq0$
  + diabetic ($>11.1$ mmol/L), non-diabetic ($\leq11.1$ mmol/L)

* Age, continuous, taking values in [0,125]
  + 10-year age bands
  
::: notes
Highest ever recorded & confirmed human age: 122 years, 164 days
:::

While sometimes useful, from a purely statistical point of view information is lost $\Rightarrow$ less statistical power, larger sample sizes.


# Exercise

## Exercise 

On the next slide are the first few rows of a longitudinal dataset monitoring systolic & diastolic blood pressure in a set of patients.

What are the types of the different variables?

```{r}
df<-data.frame(
  SampID=c("S11","S12","S13","S21","S22","S31","S32","S33"),
  PatID=c(rep("P1",3),rep("P2",2),rep("P3",3)),
  Visit=c(1:3,1:2,1:3),
  Age=c(21,31,41,28,38,26,36,46),
  Sex=c(rep("F",3),rep("M",2),rep("M",3)),
  SystBP=c(105,104,110,132,141,115,124,129),
  DiasBP=c(70,72,71,81,88,78,80,82),
  BPgroup=NA
)
df$BPgroup<-ifelse(df$SystBP<120 & df$DiasBP<80,"normal",ifelse(df$SystBP>=140 | df$DiasBP>=90,"hyper","pre-hyper"))

kable(df)
```

::: notes
SampID = categorical - nominal

PatID = categorical - nominal

Visit = cateogrical - ordinal (in fact here it may be quantitative - discrete as visits seem to be spaced exactly 10 years apart)

Age = quantitative - continuous - ratio (even if recorded only in full years)

Sex = categorical - nominal (in fact binary)

SystBP = quantitative - continuous - ratio

DiasBP = quantitative - continuous - ratio

BPgroup = categorical - ordinal
:::


# Summary & exploratory statistics

## Summary & exploratory statistics

When presented with a new dataset it is important to familiarise yourself with it.

Crucially, you want to get a feel for the data: data format, how many observations, how many reported variables, empirical distributions for important variables, numbers & patterns of missing values, ...

This will help you to know what statistical tests and models can be used for this dataset and any issues that you may face during the data analysis.


## Counts / frequencies

For categorical and discrete variables with a manageable number of different levels / values, a simple table (or graph - see later in this lecture) giving the frequency of each value in the dataset is a very intuitive and clear summary description of that variable.

It is usually helpful to not only list the frequencies but also the percentage, out of all observations, for each level.

```{r}
set.seed(24680)
typesTmp<-c("unexposed, uninfected","exposed, uninfected","infected")
status<-factor(sample(typesTmp,
                    size=750,
                    replace=T,
                    prob=c(0.56,0.21,0.23)),levels=typesTmp)
sex<-sample(size=750,x=c("F","M"),prob=c(0.75,0.25),replace=T)
age<-as.integer(10+rpois(750,lambda=ifelse(status=="infected",4,2)))
df<-data.frame(ID=paste(sep="","pid",1:length(status)),status,sex,age)
df$age[df$age==19]<-20

tt<-data.frame(sex=levels(df$sex),freq=as.matrix(table(df$sex))[,1])
tt$percentage<-paste(sep="",format(nsmall=2,round(digits=2,100*tt$freq/sum(tt$freq))),"%")
kable(tt,row.names=F)

tt<-df %>%
  count(age) %>%
  complete(age=full_seq(age,1),fill=list(n=0))
colnames(tt)[2]<-"freq"
tt$percentage<-paste(sep="",format(nsmall=2,round(digits=2,100*tt$freq/sum(tt$freq))),"%")
kable(tt,row.names=F)

tt<-data.frame(status=levels(df$status),freq=as.matrix(table(df$status))[,1])
tt$percentage<-paste(sep="",format(nsmall=2,round(digits=2,100*tt$freq/sum(tt$freq))),"%")
kable(tt,row.names=F)

g1<-ggplot(data=df,mapping=aes(x=sex)) +
  geom_bar() +
  theme(text = element_text(size=28))

g2<-ggplot(data=df,mapping=aes(x=age)) +
  geom_bar() +
  theme(text = element_text(size=28))

df2<-df
levels(df2$status)<-gsub(levels(df2$status),pattern=", ",replacement="\n")
g3<-ggplot(data=df2,mapping=aes(x=status)) +
  geom_bar() +
  theme(text = element_text(size=28))

g4<-df %>%
  count(status,age) %>%
  complete(status,age,fill=list(n=0)) %>%
  ggplot(mapping=aes(fill=status,y=n,x=age)) +
  scale_fill_manual(values=c("steelblue","salmon","orange")) +
  geom_bar(position="dodge",stat="identity") +
  theme(text = element_text(size=28))
```


## Counts / frequencies

For datset with categorical variables only, count tables can be a very concise way of storing / displaying an entire dataset.

For example, the dataset recording age status (child / adult), sex (male / female), class of travel (1^st^, 2^nd^, 3^rd^ or crew) and survival status (alive / dead) can be recorded as 2,201 individual records:

```{r, warning=F}
tit<-read.csv("titanic.csv")[,-1]

titLong<-matrix(nrow=0,ncol=4)
colnames(titLong)<-c("class","age","sex","survival")
for(j in 1:nrow(tit)){
  titLong<-rbind(titLong,matrix(byrow=T,ncol=4,rep(c(as.matrix(tit[j,c("class","age","sex")]),"alive"),tit$survivors[j])))
  titLong<-rbind(titLong,matrix(byrow=T,ncol=4,rep(c(as.matrix(tit[j,c("class","age","sex")]),"dead"),tit$dead[j])))
}

titLong<-as.data.frame(titLong)
titLong<-titLong[sample(1:nrow(titLong),size=nrow(titLong),replace=F),]

head(titLong,n=6,row.names=F)
```


## Counts / frequencies

But the entire dataset can also be shown as 14 rows of data:

```{r}
print(tit[1:10,])
```

## Counts / frequencies

```{r}
print(tit[11:14,])
```

## Tabular data / contingency tables

A very useful way to summarise two categorical variables together is to produce a **cross-tabulation table**, also called a **contingency table**.

Apart from summarising 2 variables simulateneously, a contingency table is also useful to assess whether the 2 variables are associated with each other and a first step in an association test (see Sections 7 & 8).


## Tabular data / contingency tables

For instance, in the Titanic dataset we can cross-tabulate class and survival:

```{r}
table(titLong$survival,titLong$class)
```

Or age and survival:

```{r}
table(titLong$age,titLong$survival)
```

## Central tendency

For any arbitrary distributions of *quantitative* variables, it is often helpful to summarise its **central tendency**: a central or typical value for this distribution. If one had to summarise an entire distribution by just one number, this number would be some measure of central trendency.

$$\,$$

There are are more than one possible measure that we can use to estimate the central tendency of a distribution. Commonly, central tendency statistics are called **averages**.

$$\,$$

We will cover the 3 most commonly used one: *mean*, *median*, *mode*.

## Central tendency

### Mean

We have already seen the mean for theoretical, parametric distributions: it is the expected value $\mu=E[X]$. For observed data $\mathbf{x}=\{x_1,\ldots,x_n\}$, we can calulate the **sample mean**:

$$\,$$

$$\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i = (x_1+\ldots+x_n)/n$$

## Central tendency

### Mean

Note that the above is the *arithmetic mean*. Unless otherwise specified, 'mean' will usually refer to the arithmetic mean. There are however two more commonly used means:

$$\,$$

* The *geometric mean*: $\left(\prod_{i=1}^nx_i\right)^{1/n}=\sqrt[n]{x_1\cdot x_2\cdot\ldots\cdot x_n}$.

$$\,$$

* The *harmonic mean*: $\frac{n}{1/x_1+\ldots+1/x_n}$.

::: notes
Geometric mean = useful for variables that are meant to be multiplied together or are exponential in nature

Harmonic mean = appropriate for averaging rates
:::


## Central tendency

### Median

The mean is a good measure of central tendency for symmetrically distributed random variables: it will be equal to the value around which the data are symmetric.

For skewly distributed data, the **median** is a better measure of central tendency: it is the value so that half of the data are less or equal than it and half of the data are greater or equal than it. In other words, it is the value separating the lesser and greater halves of the dataset.

Formally, for a random variable $X$, the median m is so that:

$$\,$$

$$m\in \Omega: P(X\leq m)=P(X\geq m)=0.5$$

## Central tendency

### Median

For observed data $\mathbf{x}=(x_1,\ldots,x_n)$, sort the data from smallest to largest: $x_{(1)}\leq x_{(2)}\leq\ldots\leq x_{(n)}$.

$$\,$$

The median is then the value with index $(n+1)/2$.

$$\,$$

If the number if observed value is odd, this is simply the middle value; if the number of values is even it is the average of $x_{(n/2)}$ and $x_{(n/2 +1)}$.


## Central tendency

### Mode


Another measure of central tendency is the **mode** of the distribution: it is the value where the distribution function (pmf or pdf) reaches its maximum.

$$\,$$

For general distributions and observed data, there is no simple formula to calculate the mode. In fact, it may even not be unique and for some pathological distributions it is not defined.


## Central tendency

```{r}
x<-seq(0,10,by=0.01)
xArea <- seq(0,qgamma(0.5,2,1),by=0.01)
yArea <- dgamma(xArea,2,1)

set.seed(1234)
samp<-rgamma(n=61,2,1)

par(mar=c(5,5,5,1))
plot(x, dgamma(x,2,1), main="central tendency measures for a Gamma(2,1) distribution", xlab="x", ylab="density p(x)", type="l", cex.lab=2.5,cex.axis=2,cex.main=2.5,lwd=4)
sbt<-as.vector(col2rgb("steelblue"))
polygon(c(0,xArea,qgamma(0.5,2,1)),c(0,yArea,0),col=rgb(sbt[1],sbt[2],sbt[3],alpha=100,maxColorValue=255),lty=0)
text(cex=2,"P(X \u2264 median)=0.5",x=2.15,y=0.04,col=rgb(sbt[1],sbt[2],sbt[3],alpha=100,maxColorValue=255),adj=c(0,0))

abline(lwd=4,col="steelblue",v=qgamma(0.5,2,1)) # median
abline(lwd=4,col="orange",v=2) # mean
abline(lwd=4,col="mediumorchid",v=1) # mode

points(x=samp,y=rep(0.01,length(samp)),pch=4,cex=3,col="black")
points(x=mean(samp),y=0.01,pch=19,col="orange",cex=3)
points(x=median(samp),y=0.01,pch=19,col="steelblue",cex=3)

legend(x=7.5,y=0.36,legend=c("mean","median","mode"),lwd=4,col=c("orange","steelblue","mediumorchid"),cex=2,bty="n",adj=c(0,0))
legend(x=7.5,y=0.27,legend=c("sampled observation","sample mean","sample median"),pch=c(4,19,19),cex=2,col=c("black","orange","steelblue"),bty="n",adj=c(0,0))
```

## Dispersion

Having a measure of central tendency, allows us to summarise one aspect of the data and/or their distribution. However, central tendency does not tell us anything about how spread out around this central value the data are.

$$\,$$

For this we need a measure of **dispersion**. The smaller the dispersion, the better the mean or median represent the data as a whole.

$$\,$$

Again, there are many such measures, but the most important ones are the *range*, the *interquartile range* and the *variance*.


## Dispersion

### Range

In statistics, the word **range** has 2 meanings:

$$\,$$

1. $\,$ In descriptive statistics, the range of a random variable $X$ is the shortest interval containing all possible values of $X$.

2. $\,$ As a measure of dispersion, range is the difference between the largest / **maximimum** and smallest / **minimum** values in an observed dataset: $range = max - min$.


## Dispersion

### Range

The minimum and maximum are not robust statistics: they can be quite extreme values if outliers are present in the data. A single, arbitrarily large data value is sufficient for the range to become arbitrarily large as well. As a consequence, the range (both as an interval and a measure of dispersion) is not robust either.

For this reason, measures that summarise the *average* or *central* spread are often preferred.

::: notes
The breakdown value for min/max is just 1 observation.
:::

## Dispersion

### Quantiles & the interquartile range

**q-quantiles** of a random variable are ordered values dividing the range of a random variable $X$ into $q$ intervals of equal probability. When $q=4$, the q-quantiles are called **quartiles**, when $q=100$, the q-quantiles are called **percentiles**.

$$\,$$

The median is a special case of a quantile: it is the only 2-quantile and it is equal to the 2^nd^ quartile and the 50^th^ percentile.


## Dispersion

### Quantiles & the interquartile range

The **interquartile range (IQR)** summarises the central dispersion:

$$\,$$

1. $\,$ In descriptive statistics, it is given by the interval from the 25^th^ to the 75^th^ percentile of the data. The 25^th^ percentile is also called the 1^st^ quartile and the 75^th^ percentile is the 3^rd^ quartile. In other words, the IQR contains the middle 50% of the data.

2. $\,$ As a measure of dispersion, the IQR is defined as the difference between the 3^rd^ quartile and the 1^st^ quartile: $IQR = Q_3 - Q_1$.

$$\,$$

The IQR is more robust that total range: at least 25% of the data need to be arbitrarily large for it to become arbitrarily large.

::: notes
The breakdown value for IQR is 25%.
:::

## Dispersion

### Standard deviation & variance

We have seen that one can calculate the expectation of a function $h(.)$ of a random variable $X$: $E[h(X)]$. If $h(x) = (x-E(X))^2=(x-\mu)^2$, then this expecation is called the **variance** of the random variable $X$. It computed the *average spread* around the mean value $\mu$. The variance is often denoted $\sigma^2$.

$$\,$$

$$\sigma^2=Var(X)=E\left[(X-E[X])^2\right]=E\left[(X-\mu)^2\right]$$


## Dispersion

### Standard deviation & variance

For observed data $\mathbf{X}=(x_1,\ldots,x_n)$, we can calculate the **sample variance**:

$$\,$$

$$s^2=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2$$
where $\bar{x}$ is the sample mean.


## Dispersion

### Standard deviation & variance

The **standard deviation** $\sigma$ is defined as the square root of the variance. The standard deviation has the advantage to be on the same scale of measurement as the random variable or the data.

$$\sigma=\sqrt{E\left[(X-E[X])^2\right]=E\left[(X-\mu)^2\right]}$$
$$\,$$

$$s=\sqrt{\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2}$$

## Confidence intervals

If we have a dataset, we can compute, say, the sample mean. If we sampled the population again to generate another dataset, we would get a slightly different value for the sample mean.

To quantify this uncertainty in our point estimate, we can compute a **confidence interval**. 

A **$100\cdot(1-\alpha)$% confidence interval** for a population parameter of interest $\theta$, is a random interval $[L,U]$ so that $P(\theta\in[L,U])=1-\alpha$.


## Confidence intervals

Imagine repeating an experiment a very lage number of times, e.g. $N=10,000$, and that each time you compute a 95% confidence interval for the same parameter. Assuming the model you use to construct the confidence intervals is correct, then you would expect that 9,500 of the confidence intervals you computed contained the true, unknown but fixed population value of $\theta$.

In other words, we are 95% sure, for each of the 95% confidence interval, that it contains the true value.


## Confidence intervals

The width of a confidence interval depends on:

$$\,$$

1. The confidence level $1-\alpha$.

2. The variance $\sigma^2$ of the data.

3. The number of sampled observations $n$.

$$\,$$

A CI is narrower the larger $\alpha$ is, the lower $\sigma^2$ is and the larger $n$ is.


## Confidence intervals

### Confidence interval for the sample mean

Let's return to the sample mean. It is a point estimate of the population mean from an observed dataset $\mathbf{x}=(x_1,\ldots,x_n)$.

If $n$ is large, as a result of a probability theory result called Central Limit Theorem, or if the data are normally distributed, then a 95% confidence interval for the mean can be constructed as

$$\,$$

$$\bar{x}\pm z_{1-\alpha/2}\frac{\sigma}{\sqrt{n}}$$
where $z_{1-\alpha/2}$ is the $100\cdot(1-\alpha/2)^{th}$ percentile of the standard normal distribution $\mathcal{N}(0,1)$. For $\alpha=0.05$, $z_{1-\alpha/2}\approx1.96$.


## Confidence intervals

### Confidence interval for the sample mean

$\sigma/\sqrt{n}$ is known as the **standard error** of the mean.

If the standard deviation $\sigma$ is unknown, it can be estimated by the sample standard deviation $s$. 

In this case, the approximate distribution of the sample mean is not $\mathcal{N}(\bar{x},\sigma^2)$ distributed, but rather $(\bar{x}-\mu)/s$ follows a $t$-distribution with $n-1$ degrees of freedom (approximately normal for large $n$).

A $100\cdot(1-\alpha)$% CI is then constructed as:

$$\bar{x}\pm t_{1-\alpha/2}\frac{s}{\sqrt{n}}$$




## Moments of random variables

For any distribution of a random variable $X$, we can define the **k^th^ moment**:

$$E\left[X^k\right]$$
Further one can define the **k^th^ central moment**, assuming the distribution of $X$ has mean $\mu$:

$$E\left[(X-\mu)^k\right]$$

And the **k^th^ standardised moment**, assuming the distribution of $X$ has mean $\mu$ and variance $\sigma^2$:

$$E\left[\left(\frac{X-\mu}{\sigma}\right)^k\right]$$

## Moments of random variables

The moments of a distribution carry information about its shape:

* 1^st^ moment = $\mu$ = **mean** / central tendency

* 2^nd^ central moment = $\sigma^2$ = **variance** / dispersion

* 3^rd^ standardised moment = $\gamma$ = **skewness** / symmetry

* 4^th^ standardised moment = $\kappa$ = **kurtosis** / peakedness or heaviness of tails

$$\,$$

(Note for kurtosis, often the *excess kurtosis* is reported which is $E\left[\left(\frac{X-\mu}{\sigma}\right)^4\right]-3$. The kurtosis for the any normal distribution is 3, so excess kurtosis shows how much heavier the tails are compared to a normal distribution.)


## Moments of random variables

3 examples:

1. $\, \mathcal{N}(0,1)$; standard normal

2. $\, t_5$; t-distribution with 5 degree of freedom

3. $\, \Gamma(2,1)$; gamma distribution with shape parameter $k=2$ and scale parameter $\theta=1$

## Moments of random variables

```{r, fig.width=20}
l<-1000; x<-seq(-8,8,length=l)
pnx<-dnorm(x)
ptx<-dt(x,df=5)
pgx<-dgamma(x,2,1)

xFull<-rep(x,3); pFull<-c(pnx,ptx,pgx)
pars<-factor(c(rep("N(0,1)",l),rep("t(5)",l),rep("Gamma(2,1)",l)))
dftmp<-tibble(x=xFull,p=pFull,pars=pars)

cols<-c("steelblue", "mediumorchid", "orange")
labs<-c(expression(paste(Gamma,"(2,1); ",mu,"=2, ",sigma^2,"=1, ",gamma,"=",sqrt(2),", ",kappa,"=6")),
        expression(paste("N(0,1); ",mu,"=0, ",sigma^2,"=1, ",gamma,"=0, ",kappa,"=3")),
        expression(paste("t"[5],"; ",mu,"=0, ",sigma^2,"=5/3, ",gamma,"=0, ",kappa,"=9")))

ggplot(data=dftmp,mapping=aes(x=x,y=p,colour=pars,fill=pars)) +
  geom_line(lwd=1.5,alpha=0.75) +
  geom_area(alpha=0.05,position="dodge") +
  scale_color_manual(values=cols) +
  scale_fill_manual(values=cols,labels=labs) +
  labs(fill="moments") + guides(colour=F) +
  ylab("density") + ggtitle("First 4 moments for 3 different distributions") + 
  theme(text = element_text(size=30)) 
```


## Missing values

Any real data that you are likely to collect will almost surely contain missing values: participants miss follow-up visits, data entry problems, unavailability of a specific test kit on a given day, ...

$$\,$$

It is important to report the amount of missing values for each variable in your dataset and per participant group (if results are to be analysed in separate groups or compared across groups).


## Missing values

### Why report?

There are 2 main effects that missing values can have on analysis results and it is for these reasons that it is important to report the scale of the missing values in your dataset:

$$\,$$

1. $\,$ Introduction bias if the missingness pattern is non-random, but the analysis assumes this.

2. $\,$ Increased variance which will impact statistical power. (Note that if simple imputation schemes are used for dealing with the missing values, variance will be under-estimated.)


## Missing values

### How to report?

List the entire number of participants or records in the dataset, then for each variable, list the number and percentage of observations that are not missing. This is best done as part of a table with descriptive statistics.

$$\,$$

Also report how, if at all, missing values have been dealt with during the analysis.


## Summary statistics tables

Having covered most commonly reported baseline statistics, what and how do your report this in practice?

This depends on your study, the data you collected and the focus and scope of your paper.

There are a few helpful recommendations on what are appropriate summaries to report for different variables.


## Summary statistics tables

### Tukey's 5 number summary

This is for quantitative, continuous variables.

$$\,$$

1. $\,$ Minimum

2. $\,$ 1^st^ quartile (25^th^ percentile)

3. $\,$ Median (2^nd^ quartile / 50^th^ percentile)

2. $\,$ 3^rd^ quartile (75^th^ percentile)

2. $\,$ Maximum


## Summary statistics tables

### 2 number summary for symmetric distributions

For quantitative, continuous variables that are symmetrically distributed, reporting the central tendency and a measure of dispersion is often sufficient. For symmetric distributions, mean = median = mode (though for any finite sample of data, these will differ slightly).

$$\,$$

1. $\,$ Mean

2. $\,$ Variance (or equivalently the standard deviation)


## Summary statistics tables

### Summaries for continuous & numeric variables

The following are recommended to be reported:

$$\,$$

1. $\,$ Total number of observations in dataset and number of observations with recorded data for the given variable.

2. $\,$ Mean & standard deviation

3. $\,$ Median & interquartile range

4. $\,$ Range


## Summary statistics tables

### Summaries for binary variables

The following are recommended to be reported:

$$\,$$

1. $\,$ Total number of observations in dataset and number of observations with recorded data for the given variable.

2. $\,$ Numbers in one or each of the 2 groups.

3. $\,$ Percentage of non-missing observations in one or each of the 2 groups.


## Summary statistics tables

### Summaries for categorical variables

The following are recommended to be reported:

$$\,$$

1. $\,$ Total number of observations in dataset and number of observations with recorded data for the given variable.

2. $\,$ Number of observations for each level of the categorical variable.

3. $\,$ Percentage of non-missing observations for each level.


## International reporting guidelines

Several international guidelines have been published on how to report study results. The two most relevant for this course are:

$$\,$$

1. $\,$ STROBE - for observational studies

2. $\,$ CONSORT - for clinical trials

$$\,$$

Many more guidelines exist. The EQUATOR NETWORK is a useful resource to identify what guidelines apply to your specific study:

[https://www.equator-network.org/](https://www.equator-network.org/).




# Activities

## Activity - reading: Chapter 2

![Woodward, M. (2014), *Epidemiology: Study Design and Data Analysis*, 3^rd^ ed., 'Chapter 2: Basic analytical procedures', pp. 23-85](Woodward_Epidemiology.jpeg)

## Activity - reading the STROBE statement and checklist


Read the STROBE statement and the 22-item checklist.

$$\,$$

von Elm, E., Altman, D.G., Egger, M., et al. (2007), *The Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) statement: guidelines for reporting observational studies*, Lancet, 370(9596):1453-1457.

PMID: 18064739

DOI: 10.1016/S0140-6736(07)61602-X

$$\,$$

[https://doi.org/10.1016/S0140-6736(07)61602-X](https://doi.org/10.1016/S0140-6736(07)61602-X)


## Activity - study some baseline tables

Download the 2 articles below and, for both, study Table 1:

* [Owolabi, M.O., et al. (2018), *Dominant modifiable risk factors for stroke in Ghana and Nigeria (SIREN): a case-control study*, Lancet Global Health, 6:e436–46, DOI 10.1016/S2214-109X(18)30002-0](http://dx.doi.org/10.1016/S2214-109X(18)30002-0)

* [Mulangu, S., et al. (2019), *A Randomized, Controlled Trial of Ebola Virus Disease Therapeutics*, New England Journal of Medicine, 381:2293-2303, DOI 10.1056/NEJMoa1910993](http://dx.doi.org/10.1056/NEJMoa1910993)

$$\,$$

* What information is presented?

* Is the table clear to you? If yes, why? If not, what would you do differently?

* One study is a clinical trial, the other an observational case-control study. Do you notice any differences in how the data are presented?


# Graphs

## Distribution graphs

```{r}
# generating some more data for some of the graphs to follow
set.seed(1234)
df<-mutate(df,
           biomarker=ifelse(status=="unexposed, uninfected",
                            rnorm(sum(df$status=="unexposed, uninfected"),mean=-2),
                            ifelse(status=="exposed, uninfected",
                                   rnorm(sum(df$status=="unexposed, uninfected"),mean=2),
                                   runif(sum(df$status=="infected")))))
```

### Discrete data: barplots

We have seen that frequency tables are a good way to summarise categorical data. Often a plot can summarise the same information clearer and more concisely.

**Barplots** assign a bar to each possible level / value of a random variable and the height of the bar corresponds to either the frequency of that value in the dataset or its relative proportion / percentage. The entire distribution is summarised in this way.

Barplots are particularly useful when the data are stratfied by a second variable.

## Distribution graphs - barplots

```{r, fig.width=22}
grid.arrange(nrow=1,g1,g2,g3)
```


## Distribution graphs - barplots

```{r}
print(g4)
```

## Distribution graphs - note on pie charts

Pie charts are another way of representing frequencies. Specifically the relative size of each slice of pie represents the overall proportion for that level of the categorical variable.

However, pie charts are not recommended: human brains are not well suited to easily compare areas and angles -- which pie charts force one to consider. Barplots almost always give a clearer picture of the distribution of a categorical random variable.

e.g. On the figure on the next slide, how confidently can you say that there are more infected than exposed & uninfected individuals in the dataset?

## Distribution graphs - note on pie charts

```{r}
ggplot(df, mapping=aes(x=factor(1), fill=factor(status))) +
  geom_bar(width = 1) +
  coord_polar("y") + 
  xlab("") +
  theme_void() + 
  scale_fill_manual(values=c("steelblue","salmon","orange")) +
  theme(text = element_text(size=28)) 
```

## Distribution graphs

### Continuous data

For continuous data, there are several ways we can produce a plot of the distribution of a particular random variable. The two most commonly used - by far - are:

$$\,$$

1. $\,$ Histograms
2. $\,$ Boxplots

## Distribution graphs

### Histograms & kernel density estimation

**Histograms** and **kernel density estimation** both give non-parametric estimates of the density function of a continuous random variable. Both depend on a *tuning parameter* that governs how detailed or smooth the resulting estimate is.

* Histograms split the range of the variable into bins of an equal, pre-specified width, then count the number of observations falling in each bin.

* Kernel density estimation puts a probability density of a fixed shape & width (the *kernels*) at the location of each observation. The density estimate is the summation of all of these individual density functions - normalised so that it integrates to 1.


## Distribution graphs

```{r, fig.width=18}
par(mar=c(5,6,4,0.5))
hist(df$biomarker,xlab="some biomarker",main="Histogram",col="grey",freq=F,breaks=50,cex.axis=2,cex.main=2,cex.lab=2,ylim=c(0,0.4),xlim=c(-6,6))
lines(density(df$biomarker,bw="SJ"),lwd=2)
legend(x="topright",lwd=2,legend=c("kernel density estimate"),cex=2,bty="n")
```


## Distribution graphs - larger bin width

```{r, fig.width=18}
par(mar=c(5,6,4,0.5))
hist(df$biomarker,xlab="some biomarker",main="Histogram",col="grey",freq=F,breaks=10,cex.axis=2,cex.main=2,cex.lab=2,ylim=c(0,0.4),xlim=c(-6,6))
lines(density(df$biomarker,bw="SJ"),lwd=2)
legend(x="topright",lwd=2,legend=c("kernel density estimate"),cex=2,bty="n")
```

## Distribution graphs

### Boxplots & violin plots

A **boxplot** makes use of the quartiles to represent the distribution of a random variable. Data are typically represented vertically, so that different groups can be more easily compared by juxtaposing their boxplots.

A **violin plot** is similar, but instead of boxes, draws kernel density estimates on each side.


## Distribution graphs

![Wickham, H. & Grolemund G., R for Data Science, O’Reilly, 2016](boxWhiskerPlot.png)

## Distribution graphs

```{r}
g5<-ggplot(data=df,mapping=aes(x=0,y=biomarker)) +
  geom_boxplot(fill="steelblue") + 
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), text=element_text(size=30)) +
  xlab("") + ylab("some biomarker")

g6<-ggplot(data=df,mapping=aes(x=0,y=biomarker)) +
  geom_violin(fill="salmon") +
  geom_boxplot(width=0.05, fill="white") + 
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), text=element_text(size=30)) +
  xlab("") + ylab("some biomarker")

grid.arrange(nrow=1,g5,g6)
```

## Covariation graphs

Often we want to get a feel for whether 2 variables are associated with each other: does the distribution of one variable impact the distribution of the other?

Depending on the type of data, several graphs can be helpful for this, at the exploratory stage:

$$\,$$

* Categorical & continuous: histograms for several groups

* Categorical & continuous: boxplots & violin plots for several groups

* Continuous & continuous: scatterplots


## Covariation graphs - histograms for several groups

```{r}
par(mar=c(5,6,4,0.5))
h<-hist(df$biomarker,breaks=50,plot=F)
col1<-col2rgb("steelblue"); col1<-rgb(col1[1],col1[2],col1[3],alpha=150,maxColorValue=255)
col2<-col2rgb("salmon"); col2<-rgb(col2[1],col2[2],col2[3],alpha=150,maxColorValue=255)
col3<-col2rgb("orange"); col3<-rgb(col3[1],col3[2],col3[3],alpha=150,maxColorValue=255)
hist(df$biomarker[df$status=="unexposed, uninfected"],xlab="some biomarker",main="Histogram",col=col1,breaks=h$breaks,ylim=range(h$counts),cex.axis=2,cex.lab=2,cex.main=2)
hist(df$biomarker[df$status=="exposed, uninfected"],col=col2,breaks=h$breaks,add=T)
hist(df$biomarker[df$status=="infected"],col=col3,breaks=h$breaks,add=T)
legend(x="topright",col=c("steelblue","salmon","orange"),pch=15,cex=2,legend=levels(df$status),bty="n")
```

## Covariation graphs - boxplots for several groups

```{r}
g<-ggplot(data=df,mapping=aes(x=status,y=biomarker,fill=status)) +
  geom_boxplot() +
  scale_fill_manual(values=c("steelblue","salmon","orange")) +
  theme(text = element_text(size=28))

print(g)
```

## Covariation graphs - violin plots for several groups

```{r}
g<-ggplot(data=df,mapping=aes(x=status,y=biomarker,fill=status)) +
  geom_violin() +
  geom_boxplot(width=0.05, fill="white") +
  scale_fill_manual(values=c("steelblue","salmon","orange")) +
  theme(text = element_text(size=28))

print(g)
```

## Covariation graphs - scatterplots

```{r}
df$biomarker2<-rexp(n=nrow(df),rate=0.2+0.6*abs(df$biomarker))

g<-ggplot(data=df,mapping=aes(x=biomarker,y=biomarker2)) + 
  geom_point(size=3) +
  theme(text = element_text(size=28)) +
  xlab("some biomarker") + ylab("another biomarker")

print(g)
```


## Time plots (longitudinal data)

For longitudinal data, where study participants are followed up over time, it is often helpful to produce a **time plot**. This is essentially a plot where the time series for each participant is plotted. Such plots are sometimes also called **spaghetti plots**.

```{r}
set.seed(1234)

n<-25
T<-6

gr<-expand.grid(visit=1:T,PID=1:n)

dfLong<-data.frame(PID=paste(sep="","pat",gr$PID),visit=gr$visit,biomarker=NA)
for(pid in levels(dfLong$PID)){
  intcpt<-rnorm(1,mean=0,sd=5)
  slope1<-rnorm(1,mean=0,sd=1)
  slope2<-rnorm(1,mean=0.75,sd=0.8)
  
  for(t in 1:T){
    dfLong$biomarker[dfLong$PID==pid & dfLong$visit==t]<-intcpt+slope1*t+slope2*t^2+rnorm(1,mean=0,sd=5)
  }
}

g<-ggplot(data=dfLong,mapping=aes(x=visit,y=biomarker,color=PID)) +
  geom_line(lwd=1.1) +
  geom_point(pch=20,size=5) +
  scale_color_discrete(labels=NULL,guide=F) +
  theme(text = element_text(size=28)) +
  ggtitle("time plot")

print(g)
```


# Activity

## Activity: exercise

Download the dataset `STA621_Session2.csv` from the course website.

Explore the dataset:

* How many observations are in the dataset?
* How many variables?
* For each variable (except `ID`), identify its type and produce a set of appropriate summary statistics.
* For each variable (except `ID`), produce an appropriate distribution graph.
* Pick a pair of variables for which it would make sense to explore whether they are associated with each other. Produce an appropriate covariation graph.
* Ignoring the missing values (i.e. just discard missing observations), compute the sample mean for variable `biomarker` and compute the corresponding 95% confidence interval.


# Measures of association

## Measures of association

Measures of association are a special type of summary statistics designed to **quantify** a relationship between 2 given variables.

**Association** is present between 2 variables, if their distributions are related, i.e. if knowing the value of one variable affects the distribution of values of the second variable.

Measures of association quantify the relationship between  2 *observed* variables. Tests for statistical significance inform as to whether such a relationship is likely to be present in the population.

There exist many measures of association. Some can be difficult to interpret. The type of variables dictates which measures are appropriate.


## Measures of association

If at least one of the 2 variables is categorical, we can only describe the *pattern* of association.

$$\,$$

If however both variables are at least ordinal, then we can define the **direction of association**:

* In a **positive association**, if one variables increases / decreases, then the other variable also tends to increase / decrease.

* In a **negative association**, if one variables increases, then the other variable tends to decrease and vice-versa.


## Measures of association

There are 2 types of measures of association: **symmetric** measures yield the same value regardless as to which variable is chosen to be the dependent and independent variable whereas **asymmetric** measures depend on the choice of dependent & independent variable.

Many measures of association are scaled so that

$$\,$$

$$
\begin{align}
0 &\equiv& \mbox{lack of association} \\
1 &\equiv& \mbox{perfect association}
\end{align}
$$

$$\,$$

In the case of ordinal, interval & ratio variables, some measures also indicate direction of association, with $+1$ indicating perfect, positive association and $-1$ perfect, negative association.


## Measures of association

```{r}
df<-data.frame(
  v1=c("Categorical","","","Ordinal","","Interval, ratio"),
  v2=c("Pearson's Phi","Cramer's V","Lambda","Gamma","Spearman's rank correlation coefficient","Correlation coefficient")
)
colnames(df)<-c("Lowest level of measurement","Measure of association")

kable(df)
```


## Categorical - categorical

when the lowest level of measurement of the 2 variables is nominal / categorical, then there are broadly 2 types of measures of association:

$$\,$$

* $\chi^2$ based measures

* proportional reduction in error based measures

$$\,$$

We will see examples of both.


## Categorical - categorical

Given 2 categorical variables $V1$ (taking $c$ different values), $V2$ (taking $r$ different values) we can compute a cross-tabulation / contingency table.

$$
\begin{align}
      & &       &        & V1     &        \\
      & & O_{11} & O_{12} & \ldots & O_{1c} \\
      & & O_{21} & O_{22} & \ldots & O_{2c} \\
 V2   & & \ldots &        & \ldots & \ldots \\
      & & O_{r1} & O_{r2} & \ldots & O_{rc} \\
\end{align}
$$

## Categorical - categorical

Define:

* $n=\sum_i\sum_j O_{ij} \qquad$ the total number of observations

* $p_{i.}=\frac{1}{n}\sum_j O_{ij} \qquad i=1,\ldots,r \qquad$ the row marginal proportions 

* $p_{.j}=\frac{1}{n}\sum_i O_{ij} \qquad j=1,\ldots,c \qquad$ the row marginal proportions 

* $E_{ij} = n\cdot p_{i.}\cdot p_{j.} \qquad i=1,\ldots,r, \; j=1,\ldots,c \qquad$ the expected counts in each cell


We can then calculate the $\chi^2$ statistic for this table:

$$\chi^2=\sum_i\sum_j\frac{\left(O_{ij}-E_{ij}\right)^2}{E_{ij}}$$
$\chi^2$ follows a chi-squared distribution with $(r-1)(c-1)$ d.f.


## Categorical - categorical

$X^2$, the **chi-squared** statistic, is itself a measure of association. However the value of $X^2$ depends on the sample size $n$, the number of rows $r$ and the number of columns $c$.

$$\,$$

For this reason, $\chi^2$ based measures of association correct for these.


## Categorical - categorical

A first $\chi^2$ based measure of association is $\phi$:

$$\phi = \sqrt{\frac{\chi^2}{n}}$$

$\phi$ scales the $\chi^2$ statistic by the sample size and takes the square root.

If there is no association between the variables, then $\phi=0$. However, the maximum value of $\phi$ depends on $r$ and $c$: the maximum $\phi$ value is $1-k$, where $k=min(r,c)$.

This makes it hard to compare $\phi$ between different pairs of variables where $n$ and/or $m$ vary.

For 2 binary variables: $\phi=\rho$, the Pearson correlation coefficient $\rho$.


## Categorical - categorical

A related measure, addresses this issue.

As before, define $k=\mbox{min}(r,c)$, then one can calculate **Cramer's V** statistic:

$$V=\sqrt{\frac{\chi^2}{n(k-1)}}$$

$$\,$$

$V$ ranges from 0 (no association) to 1 (perfect association), but values between 0 and 1 are difficult to interpret.


## Categorical - categorical

For nominal data, the most commonly used measure of proportional reduction in error is Goodman & Kruskal's $\lambda$:

$$\,$$

$$
\lambda=\frac{E_1-E_2}{E_1}
$$

where

* $E_1$ is the overall non-modal frequency of the first / dependent variable

* $E_2$ is the sum of the non-modal frequencies of the first / dependent variable for each level of the second / independent variable


$\lambda$ is an example of an asymmetric measure of association.

Note: $\lambda$ can be zero despite an obvious association if the modal frequency is the same at all levels of the independent variable.


## Categorical - categorical

Example

Simulated dataset on 82 lung cancer patients, 200 unrelated, healthy controls and their smoking habits.

Given the contingency table, compute $\chi^2$, $\phi$, $V$, $\lambda$.


```{r}
df<-data.frame(
  smoker=c(rep("neverSmoker",38),rep("pastSmoker",23),rep("currentSmoker",21),rep("neverSmoker",164),rep("pastSmoker",17),rep("currentSmoker",19)),
  lungCancer=c(rep("noLungCancer",200),rep("lungCancer",82)),
  stringsAsFactors=F
)
df$smoker<-factor(df$smoker,levels=c("neverSmoker","pastSmoker","currentSmoker"))
df$lungCancer<-factor(df$lungCancer,levels=c("noLungCancer","lungCancer"))

tt<-table(df$lungCancer,df$smoker)

print(tt)

x2<-chisq.test(tt)$statistic
phi<-sqrt(x2/nrow(df))
V<-sqrt(x2/(nrow(df)*1))
e1<-sum(df$lungCancer=="lungCancer")
e2<-sum(df$lungCancer=="lungCancer" & df$smoker=="neverSmoker") + sum(df$lungCancer=="lungCancer" & df$smoker=="pastSmoker") + sum(df$lungCancer=="noLungCancer" & df$smoker=="currentSmoker")
lambda<-(e1-e2)/e2
```

## Categorical - categorical

Example - Solution

$$\,$$

```{r}
dfSol<-data.frame(
  X2=x2,
  phi=phi,
  V=V,
  lambda=lambda
)
rownames(dfSol)<-NULL

print(dfSol,row.names = F)
```


## Ordinal - Ordinal

$$\,$$

For ordinal data, we have more information: we can order observations for an ordinal variable.

$$\,$$

Measures of association for ordinal data are therefore based on the **rank** of observations in each of the 2 variables.

$$\,$$

Both measures that we cover here are proportional reduction in error measures.


## Ordinal - Ordinal

For any 2 *observations* in the data, we can define them to be 

* A **concordant pair** if their order is the same in both variables

* A **discordant pair** if their order is reversed across the two variables

Now it is clear that if there exists

* A **positive** association between the variables: then the dataset will contain a lot of concordant pairs.

* A **negative** associations between the variables: then the dataset will contain a lof of discordant pairs. 

* No association: then there will be, on average, equal numbers of concordant and discordant pairs.


## Ordinal - Ordinal

This is what underlies **Goodman & Kruskal's Gamma** statistic:

$$\,$$

$$
G=\frac{n_c-n_d}{n_c+n_d}
$$
$$\,$$

where $n_c$ is the number of concordant pairs for the given set of 2 variables, $n_d$ the number of discordant pairs.


## Ordinal - Ordinal

G is obviously a symmetric measure of association.

To calculate G in practice:

* Build the contingency table where top to bottom and left to right indicates smaller to larger values.

* For $n_c$: one can form a concordant pair between an observation in a given cell with any observation in a cell above and to the left of it. Start at the bottom left and work your way up, summing the multiplications of the pairs of cells.

* For $n_d$: one can form a discordant pair between an observation in a given cell with any observation in a cell above and to the right of it. start at the bottom right and work your way up, summing the multiplications of the pairs of cells.

$G$ becomes unwieldy for ordinal variables with a wide range of possible values. 


## Ordinal - Ordinal

A very widely used measure of association is **Spearman's rank correlation coefficient**.

This measure computes the rank for each observation in each of the 2 variables. It then calculates the difference in ranks $d_i$ between the 2 variables for each observation $i=1,\ldots,n$.

Spearman's $\rho$ is then obtained as:

$$
\rho=1-\frac{6\sum_i d_i^2}{n(n^2-1)}
$$

Observations that are tied in rank for a variable are assigned the average rank between them.

$\rho$ can also be obtained by calculating Pearson's correlation coefficient on the ranks of observations in each variable.


## Ordinal - Ordinal

Example

Calculate Goodman & Kruskal's Gamma & Spearman's $\rho$ for the following data:

$$\,$$

```{r}
df<-data.frame(
  diabetic=c(rep("no diabetes",138),rep("diabetes",45)),
  BMI=c(rep("underweight",31),rep("normal",55),rep("overweight",32),rep("obese",20),rep("underweight",2),rep("normal",5),rep("overweight",19),rep("obese",19)),
  stringsAsFactors = F
)
df$diabetic<-factor(df$diabetic,levels=c("no diabetes","diabetes"))
df$BMI<-factor(df$BMI,levels=c("underweight","normal","overweight","obese"))

tt<-table(df$BMI,df$diabetic)

print(tt)
```


## Ordinal - Ordinal

Example - Solution

$$\,$$

```{r}
nc<-tt[4,2]*sum(tt[3:1,1])+tt[3,2]*sum(tt[2:1,1])+tt[2,2]*tt[1,1]#19*(32+55+31)+19*(55+31)+5*31
nd<-tt[4,1]*sum(tt[3:1,2])+tt[3,1]*sum(tt[2:1,2])+tt[2,1]*tt[1,2]#20*(19+5+2)+32*(5+2)+55*2
G<-(nc-nd)/(nc+nd)

rho<-cor(ifelse(df$diabetic=="diabetes",1,0),ifelse(df$BMI=="underweight",1,ifelse(df$BMI=="normal",2,ifelse(df$BMI=="overweight",3,4))),method="spearman")

dfSol<-data.frame(G=G,rho=rho)

print(dfSol,row.names=F)
```


## Interval / ratio - interval / ratio

For interval and ratio data, one usually refers to **correlation** rather than association.

The most common measure of correlation is **Pearson's correlation coefficient**. For 2 observed variables $\mathbf{x}$ and $\mathbf{y}$, each a vector of length $n$, it is defined as

$$
r=\frac{\sum_i(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_i(x_i-\bar{x})^2\sum_i(y_i-\bar{y})^2}}
$$
Which can also be obtained as the ratio of the sample covariance divided by the product of the sample standard deviations.


## Interval / ratio - interval / ratio

Pearson's $r$ is closely related to linear regression and is, in fact, just the standardised regression coefficient between both variables.

As such $r$ measures *linear* correlation and examples can be generated where variables are clearly related, but highly non-linearly and have an $r$ value close to 0.

$r^2$ is also called the **coefficient of determination** and can be interpreted as the proportion of variance of one variable that is explained by the other variable.

For this reason, $r$ is also a measure of proportional reduction in error.

$r$ and its relation to linear regression will be covered in more detail in the linear regression & GLM lecture.


## Interval / ratio - interval / ratio

```{r}
set.seed(111)
x<-rnorm(50)
y<-1.25*x+rnorm(50)
z<-0.5*x^2+rnorm(50,sd=0.15)

mody<-lm(y~x,data=df)
modz<-lm(z~x,data=df)

coefy<-coef(mody)
coefz<-coef(modz)

ry<-sum((x-mean(x))*(y-mean(y)))/sqrt(sum((x-mean(x))^2)*sum((y-mean(y))^2))
rz<-sum((x-mean(x))*(z-mean(z)))/sqrt(sum((x-mean(x))^2)*sum((z-mean(z))^2))

df<-data.frame(x=x,y=y,z=z)

par(mfrow=c(1,2))
plot(y~x,data=df,pch=20,cex=2)
abline(a=coefy[1],b=coefy[2],lwd=4,col="steelblue")
text(x=1.5,y=1.6,adj=c(0,0),paste(sep="","r = ",round(digits=2,ry)),font=2,col="steelblue",cex=2)
plot(z~x,data=df,pch=20,cex=2)
abline(a=coefz[1],b=coefz[2],lwd=4,col="greenyellow")
text(x=1.7,y=0.8,adj=c(0,0),paste(sep="","r = ",round(digits=2,rz)),font=2,col="greenyellow",cex=2)
```

## What when a a mix of variable types

Usually one picks the measure of association for the lowest level of measurement of both variables.

$$\,$$

Some special cases, e.g. **point bi-serial correlation** for a dichotomous and a continuous variable.


##

[end of Session 2]
