---
title: "MSc Bioinformatics - Machine learning primer"
author: "Marc Henrion"
date: "17 January 2020"
output:
  powerpoint_presentation:
    reference_doc: COM_Stats_Template.pptx
---

```{r setup, include=FALSE, echo=F}
knitr::opts_chunk$set(echo = TRUE, fig.width=16, fig.height=9, dpi=150, highlight=T)

require(tidyverse)
require(knitr)
require(gridExtra)
library(RColorBrewer)
library(MASS)
library(rms) # for splines
library(glmnet) # for ridge, lasso, elastic net
library(caret) # for various methods
library(mvtnorm) # for computing bivariate normal density
```

# Session 8: Machine learning primer

## Preliminaries

In this session we will have a very wuick look at some machine learning tools. We will focus primarily on a selection of tools for data exploration, clustering and classification / regression.

$$\,$$

* Participant packs
  + [https://github.com/gitMarcH/COM_STA621_2020](https://github.com/gitMarcH/COM_STA621_2020)
  + Copy of slides & some R codes.

* Note
  + These slides have been generated by using R Markdown.


## Useful references


:::::: {.columns}
::: {.column width="50%"}
![Hastie, T., Tibshirani, R., Friedman, J., *The elements of statistical learning*, 2^nd^ ed., 2009, Springer](images/hastietibshiranifriedman.jpg)
:::

::: {.column width="50%"}
![Bishop, C.M., *Pattern recognition and machine learning*, 2006, Springer](images/bishop.jpg)
:::
::::::


## Outline

* Data exploration: PCA & MDS
* Artificial intelligence, machine learning, statistics
* Clustering
* Classification & regression: 10 key techniques
* Boostrap & resampling
* Tools / further reading


# Data exploration: PCA & MDS

## PCA & MDS

You have multivariate data wit $p>2$ variables. You want to see whether some of your covariates drive some of the variation.
In other words: do we see clusters / patterns in the data?


$$\,$$
The problem: how do you look at high-dimensional data?


## PCA & MDS

A first idea:

We can compute distances (for various distance metrics) between samples. Obviously one can also compute distances in 1-dimensional, 2-dimensional, ... space.

$$\,$$
Could we compute distances in the p-dimensional space, and then find a 2-dimensional projection that preserves these distances?

$$\,$$
This is what **multidimensional scaling** (MDS) attempts to do.


## PCA & MDS

Mathematically to do MDS, you start from a distance matrix, reconstruct a coordinate matrix, then do an eigenvalue decomposition.
This is also what another technique, **Principle Component Analysis** (PCA) will do.

$$\,$$

Both are part of a larger set of techniques to perform dimensionality reduction. MDS can be generalised, but in its classical form both MDS and PCA are *linear* dimensionality reduction techniques.


## PCA & MDS

In **PCA**, you compute the sample covariance matrix, then do an eigenvalue decomposition of this matrix. You will end up with with a new set of *p* features that are just the projections of the data onto the ordered eigenvectors of the covariance matrix (ordered from largest to smallest eigenvalue).

$$\,$$
This first new feature, the first principle component, will explain most of the variance among the new features, the second the next most and so on. Each pair of components are *orthogonal* to each other and explain different aspects of the variation in the data.


## PCA & MDS

In an ideal world therefore, all of the "signal" in your data is on the first few principal components, whereas all the random noise is on the last few principal components.

To interpret the new coordinates, you can look at the *factor loadings*.

$$\,$$

Important application: identify ancestry / ethnicity from NGS data.


## PCA & MDS

![PCA for GWAS ancestry ( https://doi.org/10.1371/journal.pone.0122589)](images/pcaGWAS.png)


## PCA & MDS

In R:
$$\,$$

MDS: `cmdscale()`

PCA: `prcomp()`

$$\,$$
```{r}
loc<-cmdscale(eurodist)
plot(loc[,1],-loc[,2],xlab="",ylab="",axes=F,type="n",main="MDS for distances between European cities",cex=2)
text(loc[,1],-loc[,2],labels=rownames(loc),cex=2)
```


# Artificial intelligence, machine learning, statistics

## AI, machine learning, statistics

$$\,$$

Artificial **general** intelligence

Ability of a machine to represent the human mind and perform any human intellectual task.

$$\,$$

Artificial **narrow** intelligence

Ability of a machine to perform a single or a few well-defined intellectual tasks extremely well.


## AI, machine learning, statistics

* Machine learning
  + Often used interchangeably with AI (technically a sub-discipline).

* Expert systems
  + Requires (i) knowledge base and (ii) reasoning engine

* Natural language processing

* Automated planning & scheduling

* ...


## AI, machine learning, statistics

Given a set of training data vectors $\mathbf{x}=\{x_1, ..., x_n\}$, learn / tune / estimate a certain parameter of an adaptive model $\mathcal{M}$.

$$\,$$

Focus is on how to instruct a computer to do this task.

$$\,$$

*"The field of machine learning is concerned with the question of how to construct computer programs that automatically improve with experience."*
(Mitchell, T.M., Machine Learning, McGraw-Hill, 1997)

$$\,$$

In practice, the goal is usually prediction.


## AI, machine learning, statistics

* Supervised
  + Classification
  + Regression

$$\,$$

* Unsupervised
  + Clustering
  + Density estimation
  + Visualisation
  + Data mining

$$\,$$

* Reinforcement learning


## AI, machine learning, statistics

$$\,$$

"When you’re fundraising, it’s AI.
 When you’re hiring, it’s ML.
 When you’re implementing, it’s logistic regression."
 
 $$\,$$
 
								(The internet.)


## AI, machine learning, statistics

* A lot of these algorithms developed outside the context of statistical science. Many are heuristics – i.e. practical, not necessarily optimal.

* Statisticians interested in knowing asymptotic properties of methods:
  + Given infinite amounts of data, are you actually going to get the right solution?
  + If not, by how far are you off?
  + How precise will your estimate be?

* Estimated parameters have statistical properties: bias, variance, …

* Much (most?) of statistics is about minimising the bias-variance trade-off.

* In other words: While an algorithm may work on a particular dataset, will it work in general with any dataset?


## AI, machine learning, statistics

```{r, echo=F}
plotCircle<-function(radius=1,center=c(0,0),add=F,fill=F,...){
  t <- seq(0,2*pi,length=1000)
  coords <- t(rbind( center[1]+sin(t)*radius, center[2]+cos(t)*radius))
  if(!add){
    plot(coords,type="l",axes=F,xlab="",ylab="",...)
  }else{
    lines(coords,...)
  }
  
  if(fill){
    polygon(coords,border=NA,...)
  }
}

plotArcherAim<-function(){
  par(mar=c(1,1,1,1))
  plotCircle(radius=7,col="gray50",lwd=2)
  plotCircle(radius=5,col="gray50",add=T,lwd=2)
  plotCircle(radius=3,col="gray50",add=T,lwd=2)
  plotCircle(radius=1,col="gray50",add=T,lwd=2,fill=T)
}

ptsNoBiasLowVar<-mvrnorm(15,mu=c(0,0),Sigma=diag(0.2,2))
ptsNoBiasHighVar<-mvrnorm(15,mu=c(0,0),Sigma=diag(3.25,2))
ptsBiasLowVar<-mvrnorm(15,mu=c(0.65,2.65),Sigma=diag(0.2,2))
ptsBiasHighVar<-mvrnorm(15,mu=c(-2,-2),Sigma=diag(3.25,2))


layout(mat=matrix(byrow=T,1:9,nrow=3),heights=c(0.2,1,1),widths=c(0.5,1,1))

par(mar=c(0,0,0,0))
plot(type="n",0:1,0:1,axes=F)

par(mar=c(0,0,0,0))
plot(type="n",0:1,0:1,axes=F,xlab="",ylab="")
text(x=0.5,y=0.2,adj=0.5,"No bias",cex=3)

par(mar=c(0,0,0,0))
plot(type="n",0:1,0:1,axes=F,xlab="",ylab="")
text(x=0.5,y=0.2,adj=0.5,"Bias",cex=3)

par(mar=c(0,0,0,0))
plot(type="n",0:1,0:1,axes=F,xlab="",ylab="")
text(x=0.5,y=0.5,adj=0.5,"Low variance",cex=3)

par(pty="s")
plotArcherAim()
points(ptsNoBiasLowVar,col="red",pch=19,cex=3)

par(pty="s")
plotArcherAim()
points(ptsBiasLowVar,col="red",pch=19,cex=3)

par(mar=c(0,0,0,0))
plot(type="n",0:1,0:1,axes=F,xlab="",ylab="")
text(x=0.5,y=0.5,adj=0.5,"High variance",cex=3)

par(pty="s")
plotArcherAim()
points(ptsNoBiasHighVar,col="red",pch=19,cex=3)

par(pty="s")
plotArcherAim()
points(ptsBiasHighVar,col="red",pch=19,cex=3)
```


## AI, machine learning, statistics

Assume that 2 random variables, $X, Y$ are related via the relationship $Y=f(X)+\epsilon$, with $\epsilon$ some random error.

One can estimate several models, each of which produces a prediction $hat{Y}$ of $Y$. The total error may be lowest not for the model with the lowest bias but with the best **bias-variance trade-off**.

One can show that the total squared error $E[(Y-\hat{Y})^2]$ for an estimator $\hat{Y}$ of $Y$ is given by:

$$\,$$

$$
\begin{align}
E[(Y-\hat{Y})^2] & = & (E[\hat{Y}]-Y)^2 & + & E[(\hat{Y}-E[\hat{Y}])^2]  \qquad +\qquad  \sigma^2_\epsilon \\
&&&& \\
\mbox{squared error} & = & \mbox{bias}^2 \quad & + & \mbox{variance }  \quad +  \quad \mbox{ irreducible error}
\end{align}
$$


## AI, machine learning, statistics

```{r, echo=F}
c<-1:9
e<-((10-c)/2)^2/2+(c-2)^2/4+1
b<-((10-c)/2)^2/2
v<-(c/2)^2/4
i<-rep(1,length(c))

par(bg="lightgrey")
plot(axes=F,xlab="model complexity",ylab="error",c,e,pch=19,col="steelblue",ylim=c(0,15),cex=2,cex.lab=1.75)
lines(c,e,lwd=0.5,col="steelblue")
points(c,b,col="orange",pch=19,cex=2)
lines(c,b,lwd=0.5,col="orange")
points(c,v,col="darkred",pch=19,cex=2)
lines(c,v,lwd=0.5,col="darkred")
points(c,i,col="white",pch=19,cex=2)
lines(c,i,lwd=0.5,col="white")
legend(x="topleft",pch=c(19,rep(20,3)),col=c("steelblue","orange","darkred","white"),legend=c("squared error","bias squared","variance","irreducible error"),bty = "n",horiz = T,cex=1.75)
```


## AI, machine learning, statistics

* Sum of squares is the most common error function because it leads to a  convex optimisation problem.

* Statisticians tend not to optimise the sum of squared errors (least squares - LS), but rather maximise the likelihood (maximum likelihood - ML) of observing a given dataset assuming a given model. This is a more general approach, though it does make additional assumptions.

* For many models, ML & LS are equivalent, for some they are not (e.g. non-normal errors in linear model).

* In machine learning, usually predictive accuracy is optimised (using, e.g. cross-validation).


## AI, machine learning, statistics

Arguably better to maximise predictive accuracy:

Consider not only how well a model fits the current dataset, but also how well it will perform on future data.

$$\,$$

But this comes at a cost:

Not all the data is used to estimate the model parameters.

$$\,$$

What should the focus be: inference or prediction?


## AI, machine learning, statistics

For classification problems with a zero-one loss function (i.e. all misclassifications equally bad), one can show that the optimal classification rule is

$$\,$$

$$\hat{C}(x)=\mbox{argmax}_{c\in G}P(c|\mathbf{x})$$

$$\,$$

This is known as the **Bayes classifier** and its error rate (the Bayes rate) is the lowest you can achieve.

The practical problem is how to estimate the probabilities $P(c|\mathbf{x})$.


## AI, machine learning, statistics

**Statistics**

* Focus on statistical inference about model parameters.
  + Do these 2 groups have different mean values?
  + Is this slope coefficient different from 0?

$$\,$$

**Machine learning**

* Focus on prediction.
  + What is most likely type of an object, given an image of it?
  + What is the most likely successful advert to display to a user?


## AI, machine learning, statistics

![Breiman, L., *Statistical Modeling: The two Cultures* (2001). Statistical Science, 16:199-231.](images/breiman2cultures.png)


## AI, machine learning, statistics

$$\,$$

$$\,$$


$$
\begin{align}
\mbox{Mathematics} & \rightarrow & \mbox{Statistics} \\
&& \\
\mbox{Computer science} & \rightarrow & \mbox{Machine learning} \\
&& \\
\mbox{Engineering} & \rightarrow & \mbox{Pattern recognition} \\
\end{align}
$$

## AI, machine learning, statistics

$$
\begin{align}
\mathbf{Statistics} & \leftrightarrow & \mathbf{Machine\ learning} \\
&& \\
\mbox{models} & \leftrightarrow & \mbox{networks, graphs} \\
&& \\
\mbox{parameters} & \leftrightarrow & \mbox{weights} \\
&& \\
\mbox{fitting, estimating} & \leftrightarrow & \mbox{learning} \\
&& \\
\mbox{test set performance} & \leftrightarrow & \mbox{generalisation} \\
&& \\
&& \\
\mbox{logistic regression} & \approx & \mbox{perceptron} \\
\end{align}
$$


# Clustering

## Clustering

MDS and PCA allow you to visualise high-dimensional data.

As discussed often this is to discover if there are patterns or clusters of your samples (and quite often you hope to look for clusters in terms of some covariates).

So whether you'll do this on the raw data or the MDS / PCA transformed data, you will need to use techniques that can discover such patterns or clusters.


## Clustering

**Clustering** refers to discovering groups / patterns / clusters of data without knowing how such clusters should be defined. This is also called **unsupervised classification**. 

If you do know group labels, then you hope to learn rules from the data on how to predict these labels. This is called **(supervised) classification** (more on this later).


## Clustering

$$\,$$
We will look at 3 techniques:

* hiearchical clustering
* K-means clustering
* (statistical) mixture models


## Clustering

**K-means clustering**

We want to cluster some dataset $D=\{\mathbf{x}_i\}$ into $K$ groups. We need to fix $K$, though nothing will stop us do try different values of $K$.

$$\,$$

Algorithm:

1. We start by randomly selecting $K$ centres for the clusters: ${\mathbf{c}_1,\ldots,\mathbf{c}_K}$
2. Assign points to cluster whose centre is nearest.
3. Update the positions of the centres of the clusters.

Repeat steps 2. and 3. until convergence.


## Clustering

**Hiearchical clustering**

You do not fix a number of clusters. Rather you group observations into clusters depending on how similar they are. You build a tree-like graph and the final step is where to "cut" the tree - this will define the clusters.


## Clustering

**Hiearchical clustering**

Algorithm: 2 approaches

1. Bottom-up / agglomerative

    + Each observation is its own cluster.
    + Merge the two closest observations together
    + Continue until all observations are in a single cluster.

2. Top-down / divisive

    + All observations are in a single cluster.
    + Choose a cluster and a split for that cluster that best separates observations that are similar to each other from others.


## Clustering

**Hiearchical clustering**

In R:
$$\,$$

`hclust()` (agglomerative)

Also many dedicated packages for different types of hiearchical clustering and similarity metrics used along with them.


## Clustering

![Hiearchical sample and gene clustering for DE analysis (unpublished iPSC data)](images/hiearchicalClusteringDE.png)    

## Clustering

**Mixture models**

We can take a formal statistical approach. Suppose we have data $D=\{\mathbf{x}_i\}$. For each observation $\mathbf{x}_i$ we can write down it's likelihood using the Theorem of Total Probability:

$$\,$$
$$
p(\mathbf{x}_i) = \sum_{k=1}^K{ P(C_i=c_k) \cdot p(\mathbf{x}_i | C_i=c_k) } = \sum_{k=1}^K{ \pi_k \cdot p(\mathbf{x}_i | C_i=c_k) }
$$


## Clustering

**Mixture models**

The likelihood of the data is then simply
$$L(\mathbf{X}|\pi_1,\ldots,\pi_K) = \prod_{i=1}^n p(\mathbf{x}_i)$$

Because the classes are unknown we need special estimation algorithms. The most commonly used algorithm for these models is the **Expectation Maximisation (EM)** algorithm.

Start with initial parameters values, then iterate until convergence:

1. Expectation step: Do a soft assignment of observations to classes.
2. Maximisation step: Optimise the weighted likelihoods for each class.

Mixture models are a large class of models and have wide applicability.


# Classification & regression


## Classification & regression

We will cover 10 key techniques for regression and / or classification:

1. GLM: linear, log-linear, logistic, Poisson, ...
2. Splines & GAM
3. Regularisation: lasso, ridge, elastic net
4. Simple classification / clustering: k-nearest neighbour, k-means
5. Kernel density estimation & classification
6. LDA, QDA, RDA
7. Decision trees & random forest
8. Neural networks & deep learning
9. Graphical models
10. Bootstrap & resampling techniques

But first, let's generate some data.


## Classification & regression

```{r genDat, echo=F, warning=F}
set.seed(12345) # for reproducibility

x<-runif(100,min=0.25,max=3)
y<-10-2/x+rnorm(length(x),mean=0,sd=1)

regDat<-data.frame(x=x,y=y)

for(j in 1:6){
  regDat[,paste(sep="","v",j)]<-rnorm(length(x)) # for the penalisation methods
}

classDat<-data.frame(class=c(rep("c1",50),rep("c2",100)),x=NA,y=NA)
classDat[classDat$class=="c1",c("x","y")]<-mvrnorm(sum(classDat$class=="c1"),mu=c(1,2),Sigma=diag(1,2))
classDat[classDat$class=="c2",c("x","y")]<-mvrnorm(sum(classDat$class=="c2"),mu=c(2,1),Sigma=matrix(byrow=T,nrow=2,c(0.5,-0.2,-0.2,0.5)))

newRegDat<-data.frame(x=seq(0.25,3,length=1000))
newClassDat<-expand.grid(x=seq(-1,4.5,length=300),y=seq(-1,4.5,length=300))

# simple data plots
p0a<-ggplot() + 
  geom_point(data=regDat,mapping=aes(x=x,y=y),size=3) +
  ggtitle("regression data") +
  ylim(c(2,12))

p0b<-ggplot() + 
  geom_point(data=classDat,mapping=aes(col=class,x=x,y=y,pch=class),size=3) + 
  xlim(-1,4.25) + ylim(-1,4.25) +
  coord_fixed(ratio = 1) +
  ggtitle("classification / clustering data")

grid.arrange(p0a,p0b,ncol=2)
```


## Classification & regression

```{r genDat2, echo=F, warning=F}
densC1<-dmvnorm(x=newClassDat,mean=c(1,2),sigma=diag(1,2))
densC2<-dmvnorm(x=newClassDat,mean=c(2,1),sigma=matrix(byrow=T,nrow=2,c(0.5,-0.2,-0.2,0.5)))
priorC1<-50/150
priorC2<-100/150
postC1<-priorC1*densC1/(priorC1*densC1+priorC2*densC2)
newClassDat$bayesClassif<-factor(ifelse(postC1>0.5,"c1","c2"))
newClassDat$bayesClassifNum<-postC1
newClassDat$trueC1dens<-densC1
newClassDat$trueC2dens<-densC2

datLine<-data.frame(
  x<-seq(0.05,3,by=0.01),
  y=NA
)
datLine$y<-10-2/datLine$x

p0awithLine<-p0a +
  geom_line(data=datLine,mapping=aes(x=x,y=y), color="blue", linetype="solid") +
  ggtitle("regression data (with actual mean relationship)")

p0bwithBayesClassif<-p0b + 
  geom_point(mapping=aes(x=x,y=y,col=bayesClassif),data=newClassDat,size=0.2,alpha=0.5) +
  geom_contour(mapping=aes(x=x,y=y,z=bayesClassifNum),data=newClassDat,breaks=0.5,color="black",lwd=0.8) + 
  geom_contour(mapping=aes(x=x,y=y,z=trueC1dens),data=newClassDat,color="red",lwd=0.5) +
  geom_contour(mapping=aes(x=x,y=y,z=trueC2dens),data=newClassDat,color="blue",lwd=0.5) +
  ggtitle("Bayes classifier (with true generating distributions) - classification boundary")

grid.arrange(p0awithLine,p0bwithBayesClassif,ncol=2)
```


## Classification & regression - 1. GLM

GLM: linear, log-linear, logistic, Poisson, ...


**Linear model**

$$Y=\beta_0+\beta_1X_1+\beta_2X_2+\ldots+\epsilon\qquad\epsilon\sim\mathcal{N}(0,\sigma^2)$$
$$\,$$

**Generalised linear model (GLM)**

Includes logistic regression, Poisson regression, ...

$$\,$$
 
$$g(E[Y|\mathbf{X}])=\beta_0+\beta_1X_1+\beta_2X_2+\ldots$$

$$Y|\mathbf{X}\sim F$$

## 1. GLM

```{r glm, echo=F, warning=F}
m1<-lm(y~x,data=regDat)
pred1<-data.frame(predict(m1,newdata=newRegDat,interval="confidence"))
newRegDat$lwr<-pred1$lwr
newRegDat$upr<-pred1$upr
newRegDat$fit<-pred1$fit
polyDat<-data.frame(x=c(newRegDat$x,newRegDat$x[nrow(newRegDat):1]),y=c(newRegDat$lwr,newRegDat$upr[nrow(newRegDat):1]))

p1reg<-p0a + 
  geom_polygon(data=polyDat,mapping=aes(x=x,y=y),color=NA,fill="darkgrey",alpha=0.5) +
  geom_line(data=newRegDat,mapping=aes(x=x,y=fit), color="blue", linetype="solid") +
  ggtitle("GLM")

print(p1reg)
```

## 1. GLM

```{r glm2, echo=F, warning=F}
m1class<-glm(family="binomial",class~x+y,data=classDat)
newClassDat$pred1class<-predict(m1class,newdata=newClassDat,type="response")
newClassDat$pred1classBin<-factor(ifelse(newClassDat$pred1class>0.5,"c2","c1"))

p1class<- p0b +
  geom_point(mapping=aes(x=x,y=y,col=pred1classBin),data=newClassDat,size=0.2,alpha=0.5) +
  geom_contour(mapping=aes(x=x,y=y,z=pred1class),data=newClassDat,breaks=0.5,color="black",lwd=0.8) + 
  ggtitle("GLM - logistic regression classification boundary")

print(p1class)
```

## 2. Splines & GAM

Splines and GAMS relax the linearity of GLMs and allow quite smooth fits.

$$\,$$

**Linear spline model**

$$Y=\beta_0+\beta_1X+\beta_2(X-k)_++\epsilon\qquad\epsilon\sim\mathcal{N}(0,\sigma^2)$$
$$\,$$

where $(X-k)_+=X-k$ if $X>k$ and $0$ otherwise.

$$\,$$

**Generalised additive model (GAM)**

$$g(E[Y|\mathbf{X}])=f_1(X_1)+f_2(X_2)+\ldots$$

$$Y|\mathbf{X}\sim F$$

## 2. Splines & GAM

```{r splinesGAMs, echo=F, warning=F}
m2a<-lm(y~lsp(x,1),data=regDat)
pred2a<-data.frame(predict(m2a,newdata=newRegDat,interval="confidence"))
newRegDat$lwr<-pred2a$lwr
newRegDat$upr<-pred2a$upr
newRegDat$fit<-pred2a$fit
polyDat<-data.frame(x=c(newRegDat$x,newRegDat$x[nrow(newRegDat):1]),y=c(newRegDat$lwr,newRegDat$upr[nrow(newRegDat):1]))

p2a<-p0a +
  geom_polygon(data=polyDat,mapping=aes(x=x,y=y),color=NA,fill="darkgrey",alpha=0.5) +
  geom_line(data=newRegDat,mapping=aes(x=x,y=fit), color="blue", linetype="solid") +
  ggtitle("Linear spline (1 knot)")

m2b<-lm(y~rcs(x,3),data=regDat)
pred2b<-data.frame(predict(m2b,newdata=newRegDat,interval="confidence"))
newRegDat$lwr<-pred2b$lwr
newRegDat$upr<-pred2b$upr
newRegDat$fit<-pred2b$fit
polyDat<-data.frame(x=c(newRegDat$x,newRegDat$x[nrow(newRegDat):1]),y=c(newRegDat$lwr,newRegDat$upr[nrow(newRegDat):1]))

p2b<-p0a +
  geom_polygon(data=polyDat,mapping=aes(x=x,y=y),color=NA,fill="darkgrey",alpha=0.5) +
  geom_line(data=newRegDat,mapping=aes(x=x,y=fit), color="blue", linetype="solid") +
  ggtitle("Restricted cubic spline (3 knots)")


grid.arrange(p2a,p2b,ncol=2)
```


## 3. Regularisation / penalisation / shrinkage

GLM – but with additional parameter constraints to shrink parameters. This controls variability and helps with variable selection.

$$\,$$

**Ridge regression** – L2 norm: $\lambda\sum_j\beta_j^2$

Coefficients are shrunk, but kept in the model.

$$\,$$

**Lasso regression** – L1 norm: $\lambda\sum_j|\beta_j|$

Coefficients are shrunk, some to exactly 0.

$$\,$$

**Elastic net** – a compromise: $\lambda\sum_j\left(\alpha\beta_j^2 + (1-\alpha)|\beta_j|\right)$


## 3. Regularisation / penalisation / shrinkage

```{r regul, echo=F, warning=F}
m3_elnet<-glmnet(y=regDat$y,x=as.matrix(regDat[,-2]),alpha=0.5)
m3_ridge<-glmnet(y=regDat$y,x=as.matrix(regDat[,-2]),alpha=0)
m3_lasso<-glmnet(y=regDat$y,x=as.matrix(regDat[,-2]),alpha=1)

par(mfrow=c(1,2),mar=c(5,4,6,1))
plot(m3_ridge,main="ridge regression",label=T,xvar="lambda",lwd=2)
plot(m3_lasso,main="lasso regression",label=T,xvar="norm",lwd=2)
```


## 4. Simple classification / clustering: kNN, k-Means

$$\,$$

These are prototype, memory-based, model-free methods.

$$\,$$

### 4.1 kNN

Assign to majority class among $k$ nearest data points in the training set.

Can also be used for regression: $y=\frac{1}{k}\sum_{x_i\in N_k}y_i$


## 4. Simple classification / clustering: kNN, k-Means

```{r kNN, echo=F, warning=F}
m4knn<-train(class ~ x + y, data = classDat,
                 method = "knn",
                 preProcess = c("center", "scale"), # (centering &) scaling recommended as otherwise variables on large numerical scales will dominate the distance metric
                 tuneGrid=data.frame(k=15)) # number of nearest neighbours; could also have R select this for us via CV but for illustration purposes we fix this here (otherwise we would first specify parameters for the CV...)

newClassDat$pred4knn<-predict(m4knn,newdata=newClassDat)
newClassDat$pred4knnNum<-as.integer(newClassDat$pred4knn)-1

p4knn<-p0b +
  geom_point(mapping=aes(x=x,y=y,col=pred4knn),data=newClassDat,size=0.2,alpha=0.5) +
  geom_contour(mapping=aes(x=x,y=y,z=pred4knnNum),data=newClassDat,breaks=0.5,color="black",lwd=0.8) + 
  ggtitle("nearest neighbour (k=15) classification boundary")

print(p4knn)

m4knnReg<-train(y ~ x, data = regDat,
             method = "knn",
             preProcess = c("center", "scale"), # (centering &) scaling recommended as otherwise variables on large numerical scales will dominate the distance metric
             tuneGrid=data.frame(k=15)) # number of nearest neighbours; could also have R select this for us via CV but for illustration purposes we fix this here (otherwise we would first specify parameters for the CV...)

newRegDat$pred4knnReg<-predict(m4knnReg,newdata=newRegDat)

p4knnReg<-p0a +
  geom_line(data=newRegDat,mapping=aes(x=x,y=pred4knnReg), color="blue", linetype="solid") +
  ggtitle("kNN regression (k=15)")

print(p4knnReg)
```


## Simple classification / clustering: kNN, k-Means

$$\,$$

These are prototype, memory-based, model-free methods.

$$\,$$

### 4.2 k-Means

1. Start with a random set of initial cluster centroids.
2. Assign samples to nearest centroid.
3. Recompute centroids for each cluster.
4. Iterate until convergence.


## 4. Simple classification / clustering: kNN, k-Means

```{r kMeans, echo=F, warning=F}
m4kmeans<-kmeans(classDat[,c("x","y")], centers=2, nstart=20) # in practice, just as for kNN, you would probably want to do first center & scale....
classDat$kmeans<-factor(m4kmeans$cluster)
classC1<-as.character(names(sort(table(classDat$class[classDat$kmeans==1]),decreasing=TRUE))[1]) # this work here, but could backfire if one class is overwhelmingly more common than the other and both clusters aredominated by that class...
classC2<-as.character(names(sort(table(classDat$class[classDat$kmeans==2]),decreasing=TRUE))[1]) # this work here, but could backfire if one class is overwhelmingly more common than the other and both clusters aredominated by that class...

nearestCentroid<-function(x,kmeans) {
  centDist<-apply(kmeans$centers, MARGIN=1, FUN=function(y){sqrt(sum((x-y)^2))})
  return(which.min(centDist)[1])
}

newClassDat$pred4kmeans<-factor(ifelse(apply(newClassDat[,c("x","y")],MARGIN=1,FUN=nearestCentroid,kmeans=m4kmeans)==1,classC1,classC2))
newClassDat$pred4kmeansNum<-as.integer(newClassDat$pred4kmeans)-1

p4kmeansClust<-ggplot() + 
  geom_point(data=classDat,mapping=aes(x=x,y=y,pch=kmeans),col="black",alpha=0.5,cex=5) + 
  geom_point(data=classDat,mapping=aes(x=x,y=y,col=class),cex=2) +
  xlim(-1,4.25) + ylim(-1,4.25) +
  ggtitle("k-means clustering")

print(p4kmeansClust)

p4kmeansClass<-p0b +
  geom_point(mapping=aes(x=x,y=y,col=pred4kmeans),data=newClassDat,size=0.2,alpha=0.5) +
  geom_contour(mapping=aes(x=x,y=y,z=pred4kmeansNum),data=newClassDat,breaks=0.5,color="black",lwd=0.8) + 
  ggtitle("k-means classification boundary")

print(p4kmeansClass)
```


## 5. kernel density estimation & classification

**Kernel density estimation** – drop a probability kernel at each observation:

$$\hat{f}(\mathbf{x})=\frac{1}{n}\sum_{i=1}^ng(\mathbf{x}-\mathbf{x}_i|\mathbf{\theta})$$

For classification, estimate densities for each class, then use Bayes’ Rule:

$$
P(C=j|\mathbf{x})=\frac{\hat{\pi}_j\hat{f}_j(\mathbf{x})}{\sum_k\hat{\pi}_k\hat{f}_k(\mathbf{x})}
$$
Note: only need to estimate densities well near the decision boundary.


## 5. kernel density estimation & classification

```{r kde, echo=F, warning=F}
dens1<-kde2d(x=classDat$x[classDat$class=="c1"],y=classDat$y[classDat$class=="c1"],n=300,lims=c(c(-1,4.25),c(-1,4.25)))
dens2<-kde2d(x=classDat$x[classDat$class=="c2"],y=classDat$y[classDat$class=="c2"],n=300,lims=c(c(-1,4.25),c(-1,4.25)))
prior1<-sum(classDat$class=="c1")/nrow(classDat)
prior2<-sum(classDat$class=="c2")/nrow(classDat)
class1Post<-prior1*dens1$z/(prior1*dens1$z+prior2*dens2$z) # only can do this because we evaluated dens1, dens2 on the same grid!
class2Post<-prior2*dens2$z/(prior1*dens1$z+prior2*dens2$z) # not needed; only added for the sake of completeness
class1PostVect<-as.vector(class1Post)

tmpGr<-expand.grid(dens1$x,dens1$y)
tmpClassDat<-data.frame(x=tmpGr[,1],y=tmpGr[,2],pred5KernDensNum=class1PostVect,pred5KernDens=factor(ifelse(class1PostVect>0.5,"c1","c2")),class1Dens=as.vector(dens1$z),class2Dens=as.vector(dens2$z))

p5kerndensClass<-p0b + 
  geom_point(mapping=aes(x=x,y=y,col=pred5KernDens),data=tmpClassDat,size=0.2,alpha=0.5) +
  geom_contour(mapping=aes(x=x,y=y,z=pred5KernDensNum),data=tmpClassDat,breaks=0.5,color="black",lwd=0.8) + 
  geom_contour(mapping=aes(x=x,y=y,z=class1Dens),data=tmpClassDat,color="red",lwd=0.5) +
  geom_contour(mapping=aes(x=x,y=y,z=class2Dens),data=tmpClassDat,color="blue",lwd=0.5) +
  ggtitle("Kernel density estimation - classification boundary")

print(p5kerndensClass)
```

## 5. kernel density estimation & classification

$$\,$$

Naïve Bayes – useful in high-dimensional problems:

$$\hat{f}(\mathbf{x})=\prod_{m=1}^p\hat{f}_{jm}(x_m)$$


##  6. LDA, QDA, RDA

As for kernel density estimation, classification is based on

$$
P(C=j|\mathbf{x})=\frac{\hat{\pi}_j\hat{f}_j(\mathbf{x})}{\sum_k\hat{\pi}_k\hat{f}_k(\mathbf{x})}
$$

But the $\hat{f}_j(\hatbf{x})$ are assumed to be normal distributions of which we estimate the parameters.

In a two-class problem, it is enough to consider the log-ratio of the posterior class probabilities.

**LDA**: assume equal covariance matrix for all classes – linear in $\mathbf{x}$

**QDA**: unequal covariance matrices – quadratic in $\mathbf{x}$

**RDA**: shrink QDA covariances to a common covariance matrix.
	

##  6. LDA, QDA, RDA

```{r discAnal, echo=F, warning=F}
m7lda<-train(class ~ x + y, data = classDat, method = "lda") 

newClassDat$pred7lda<-predict(m7lda,newdata=newClassDat)
newClassDat$pred7ldaNum<-as.integer(newClassDat$pred7lda)-1

m7qda<-train(class ~ x + y, data = classDat, method = "qda") 

newClassDat$pred7qda<-predict(m7qda,newdata=newClassDat)
newClassDat$pred7qdaNum<-as.integer(newClassDat$pred7qda)-1

p7lda<-p0b +
  geom_point(mapping=aes(x=x,y=y,col=pred7lda),data=newClassDat,size=0.2,alpha=0.5) +
  geom_contour(mapping=aes(x=x,y=y,z=pred7ldaNum),data=newClassDat,breaks=0.5,color="black",lwd=0.8) + 
  ggtitle("LDA classification boundary")

p7qda<-p0b +
  geom_point(mapping=aes(x=x,y=y,col=pred7qda),data=newClassDat,size=0.2,alpha=0.5) +
  geom_contour(mapping=aes(x=x,y=y,z=pred7qdaNum),data=newClassDat,breaks=0.5,color="black",lwd=0.8) + 
  ggtitle("QDA classification boundary")

grid.arrange(p7lda,p7qda,ncol=2)
```



## 7. Decision trees & random forest

Decision trees aim to partition the feature space into sub-regions, with a simple model in each region:

* Average value of response variable in region $R_m$ (regression).
* Majority class in region $R_m$ (classification).

$$\,$$

At each step the algorithm selects a variable and a split on that variable that minimizes a node impurity function.

* Squared error node impurity (regression).
* Misclassification rate / Gini coefficient / deviance (classification).

$$\,$$

To avoid overfitting a cost-complexity function is optimised that penalises large trees.


## 7.1 CART

```{r cart, echo=F, warning=F}
m8cartClass<-train(class ~ x + y, data = classDat, method = "rpart", tuneGrid=data.frame(cp=0)) # as this is a simple example, the graph will be more interesting for a fully grown tree, hence set cost-complexiy parameter to 0

newClassDat$pred8cartClass<-predict(m8cartClass,newdata=newClassDat)
newClassDat$pred8cartClassNum<-as.integer(newClassDat$pred8cartClass)-1

p8cartClass<-p0b +
  geom_point(mapping=aes(x=x,y=y,col=pred8cartClass),data=newClassDat,size=0.2,alpha=0.5) +
  geom_contour(mapping=aes(x=x,y=y,z=pred8cartClassNum),data=newClassDat,breaks=0.5,color="black",lwd=0.8) + 
  ggtitle("CART decision tree (fully grown) classification boundary")

m8cartReg<-train(y ~ x, data = regDat, method = "rpart")
newRegDat$pred8cartReg<-predict(m8cartReg,newdata=newRegDat)

p8cartReg<-p0a +
  geom_line(data=newRegDat,mapping=aes(x=x,y=pred8cartReg), color="blue", linetype="solid") +
  ggtitle("CART decision tree regression")

grid.arrange(p8cartClass,p8cartReg,ncol=2)
```


## 7. Decision trees & random forest

* Decision trees are highly
  + Interpretable.
  + Unstable (high variance).

$$\,$$

* Random forest tries to overcome the latter at the cost of the former:
  + Draw $B$ bootstrap samples.
  + Grow a tree for each bootstrapped dataset
  + Randomly select $m$ out of $p$ variables.
  + Pick best variable & split among the m variables.
  + Split the node.

$$\,$$

* Predict
  + Regression: average predicted value among the $B$ trees
  + Classification: majority vote among the $B$ trees


## 7.2 Random forest

```{r rf, echo=F, warning=F}
m8rfClass<-train(class ~ x + y, data = classDat, method = "rf", tuneGrid=data.frame(mtry=c(1,2)), trainControl=trainControl(method = "repeatedcv", number = 10, repeats = 3))

newClassDat$pred8rfClass<-predict(m8rfClass,newdata=newClassDat,type="raw")
newClassDat$pred8rfClassNum<-predict(m8rfClass,newdata=newClassDat,type="prob")[,1]

p8rfClass<-p0b +
  geom_point(mapping=aes(x=x,y=y,col=pred8rfClass),data=newClassDat,size=0.2,alpha=0.5) +
  geom_contour(mapping=aes(x=x,y=y,z=pred8rfClassNum),data=newClassDat,breaks=0.5,color="black",lwd=0.8) + 
  ggtitle("Random forest classification boundary")

m8rfReg<-train(y ~ x, data = regDat, method = "rf", tuneGrid=data.frame(mtry=1), trainControl=trainControl(method = "repeatedcv", number = 10, repeats = 3))
newRegDat$pred8rfReg<-predict(m8rfReg,newdata=newRegDat)

p8rfReg<-p0a +
  geom_line(data=newRegDat,mapping=aes(x=x,y=pred8rfReg), color="blue", linetype="solid") +
  ggtitle("Random forest regression")

grid.arrange(p8rfClass,p8rfReg,ncol=2)
```


## 8. Neural networks & deep learning

Developed independently in the fields of statistics (projection pursuit) and artificial intelligence.

$$\,$$

The basic idea is: form non-linear functions of linear combination of non-linear functions of linear combinations of ...

$$\,$$

This results in a *universal approximator*: for large enough numbers of nodes in the model, any continuous function can be approximated arbitrarily well.

$$\,$$

Large class of models (focus here on the **multilayer perceptron** - MLP).

## 8. Neural networks & deep learning

![The perceptron and a multilayer perceptron.](images/nnets.png)

## 8. Neural networks & deep learning

```{r nnets, echo=F, warning=F}
# sticking to basic MLPs here
m9mlpClass<-train(class ~ x + y, data = classDat, method = "mlp") # many other methods, e.g. nnet
newClassDat$pred9mlpClass<-predict(m9mlpClass,newdata=newClassDat,type="raw")
newClassDat$pred9mlpClassNum<-predict(m9mlpClass,newdata=newClassDat,type="prob")[,1]

p9mlpClass<-p0b +
  geom_point(mapping=aes(x=x,y=y,col=pred9mlpClass),data=newClassDat,size=0.2,alpha=0.5) +
  geom_contour(mapping=aes(x=x,y=y,z=pred9mlpClassNum),data=newClassDat,breaks=0.5,color="black",lwd=0.8) + 
  ggtitle("Neural network (MLP, 1 hidden layer) classification boundary")


m9mlpReg<-train(y ~ x, data = regDat, method = "mlp")
newRegDat$pred9mlpReg<-predict(m9mlpReg,newdata=newRegDat)

p9mlpReg<-p0a +
  geom_line(data=newRegDat,mapping=aes(x=x,y=pred9mlpReg), color="blue", linetype="solid") +
  ggtitle("Neural network (MLP, 1 hidden layer) regression")

grid.arrange(p9mlpClass,p9mlpReg,ncol=2)
```


## 9. Graphical models

A large class of models, that uses graphs (and graph theory) to represent the joint distribution of sets of random variables.

$$\,$$

2 broad families:

* Bayesian networks (directed acyclic graphs)
* Markov graphs (undirected graphs)


## 10. Bootstrap & resampling techniques

This is a set of techniques to use the data itself, to do (usually) one of 3 things:

$$\,$$

* Estimate precision of estimated parameters.
  + Bootstrap (draw a new set of same size from the data with replacement)
  + Jackknife (systematic subsets of size n-1)

$$\,$$

* Perform statistical tests via permutation.
  + Permutation tests (permute labels)

$$\,$$

* Assess model performance.
  + Cross-validation, leave-one-out (draw random subsets)
  + Bootstrap


# Tools / further reading

## Tools / further reading

$$\,$$

R package & library: `caret`

$$\,$$

WEKA: [https://www.cs.waikato.ac.nz/ml/weka/](https://www.cs.waikato.ac.nz/ml/weka/)

$$\,$$

TensorFlow: [https://www.tensorflow.org/](https://www.tensorflow.org/)


##

$$\,$$

$$\,$$

[end of Session 8]
